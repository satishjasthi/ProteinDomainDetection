{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/IPython/utils/traitlets.py:5: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  warn(\"IPython.utils.traitlets has moved to a top-level traitlets package.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProjectRoot: /home/satish27may/ProteinDomainDetection\n",
      "Number of records dropeed after selecting major domains based on len: 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config /home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/detectron2/model_zoo/configs/COCO-Detection/../Base-RetinaNet.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
      " 40%|████      | 468/1163 [00:00<00:00, 4676.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes model is seeing: 1\n",
      "Train data dist:\n",
      "\n",
      "Counter({'PF05257': 1190})\n",
      "Valid data dist:\n",
      "\n",
      "Counter({'PF05257': 507})\n",
      "Registring train data.......\n",
      "Classes selected: ['CHAP-PF05257']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1163/1163 [00:00<00:00, 4732.71it/s]\n",
      "100%|██████████| 499/499 [00:00<00:00, 5084.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registring valid data.......\n",
      "Classes selected: ['CHAP-PF05257']\n",
      "\u001b[32m[12/11 14:15:32 d2.engine.defaults]: \u001b[0mModel:\n",
      "RetinaNet(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelP6P7(\n",
      "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): RetinaNetHead(\n",
      "    (cls_subnet): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (bbox_subnet): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (cls_score): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (anchor_generator): DefaultAnchorGenerator(\n",
      "    (cell_anchors): BufferList()\n",
      "  )\n",
      ")\n",
      "\u001b[32m[12/11 14:15:32 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 1163 images left.\n",
      "\u001b[32m[12/11 14:15:32 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|   category   | #instances   |\n",
      "|:------------:|:-------------|\n",
      "| CHAP-PF05257 | 1190         |\n",
      "|              |              |\u001b[0m\n",
      "\u001b[32m[12/11 14:15:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1330, sample_style='choice'), RandomFlip(horizontal=False, vertical=True)]\n",
      "\u001b[32m[12/11 14:15:32 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[12/11 14:15:32 d2.data.common]: \u001b[0mSerializing 1163 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[12/11 14:15:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.35 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'head.cls_score.weight' to the model due to incompatible shapes: (720, 256, 3, 3) in the checkpoint but (9, 256, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'head.cls_score.bias' to the model due to incompatible shapes: (720,) in the checkpoint but (9,) in the model! You might want to double check if this is expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/11 14:15:34 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/11 14:15:44 d2.utils.events]: \u001b[0m eta: 0:00:04  iter: 19  total_loss: 1.71  loss_cls: 1.278  loss_box_reg: 0.4319  time: 0.4518  data_time: 0.0432  lr: 1.7781e-05  max_mem: 5636M\n",
      "\u001b[32m[12/11 14:15:49 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 29  total_loss: 1.305  loss_cls: 0.9851  loss_box_reg: 0.3299  time: 0.4425  data_time: 0.0164  lr: 2.4628e-07  max_mem: 5636M\n",
      "\u001b[32m[12/11 14:15:49 d2.engine.hooks]: \u001b[0mOverall training speed: 28 iterations in 0:00:12 (0.4425 s / it)\n",
      "\u001b[32m[12/11 14:15:49 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:13 (0:00:00 on hooks)\n",
      "\u001b[32m[12/11 14:15:49 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|   category   | #instances   |\n",
      "|:------------:|:-------------|\n",
      "| CHAP-PF05257 | 507          |\n",
      "|              |              |\u001b[0m\n",
      "\u001b[32m[12/11 14:15:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1330, sample_style='choice')]\n",
      "\u001b[32m[12/11 14:15:49 d2.data.common]: \u001b[0mSerializing 499 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[12/11 14:15:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.15 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[12/11 14:15:49 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[12/11 14:15:49 d2.evaluation.coco_evaluation]: \u001b[0m'valid' is not registered by `register_coco_instances`. Therefore trying to convert it to COCO format ...\n",
      "\u001b[32m[12/11 14:15:49 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'valid' to COCO format ...)\n",
      "\u001b[32m[12/11 14:15:49 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n",
      "\u001b[32m[12/11 14:15:49 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 499, #annotations: 507\n",
      "\u001b[32m[12/11 14:15:49 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at '/home/satish27may/ProteinDomainDetection/models/CHAP-PF05257_0_300/valid_coco_format.json' ...\n",
      "\u001b[32m[12/11 14:15:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1330, sample_style='choice')]\n",
      "\u001b[32m[12/11 14:15:50 d2.data.common]: \u001b[0mSerializing 499 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[12/11 14:15:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.15 MiB\n",
      "\u001b[32m[12/11 14:15:50 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1330, sample_style='choice')]\n",
      "\u001b[32m[12/11 14:15:50 d2.data.common]: \u001b[0mSerializing 499 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[12/11 14:15:50 d2.data.common]: \u001b[0mSerialized dataset takes 0.15 MiB\n",
      "\u001b[32m[12/11 14:15:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 499 images\n",
      "\u001b[32m[12/11 14:15:50 d2.evaluation.evaluator]: \u001b[0mInference done 11/499. 0.0285 s / img. ETA=0:00:14\n",
      "\u001b[32m[12/11 14:15:55 d2.evaluation.evaluator]: \u001b[0mInference done 178/499. 0.0282 s / img. ETA=0:00:09\n",
      "\u001b[32m[12/11 14:16:00 d2.evaluation.evaluator]: \u001b[0mInference done 346/499. 0.0282 s / img. ETA=0:00:04\n",
      "\u001b[32m[12/11 14:16:05 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:14.851617 (0.030064 s / img per device, on 1 devices)\n",
      "\u001b[32m[12/11 14:16:05 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:13 (0.028173 s / img per device, on 1 devices)\n",
      "\u001b[32m[12/11 14:16:05 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[12/11 14:16:05 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to /home/satish27may/ProteinDomainDetection/models/CHAP-PF05257_0_300/coco_instances_results.json\n",
      "\u001b[32m[12/11 14:16:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.19s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "COCOeval_opt.evaluate() finished in 0.13 seconds.\n",
      "Accumulating evaluation results...\n",
      "COCOeval_opt.accumulate() finished in 0.03 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.059\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.143\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.037\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.075\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.071\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.472\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.550\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.550\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      "\u001b[32m[12/11 14:16:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 5.938 | 14.302 | 3.723  |  nan  | 7.504 |  nan  |\n",
      "\u001b[32m[12/11 14:16:06 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[12/11 14:16:06 d2.engine.defaults]: \u001b[0mEvaluation results for valid in csv format:\n",
      "\u001b[32m[12/11 14:16:06 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[12/11 14:16:06 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[12/11 14:16:06 d2.evaluation.testing]: \u001b[0mcopypaste: 5.9379,14.3016,3.7232,nan,7.5044,nan\n",
      "OrderedDict([('bbox', {'AP': 5.937924725952366, 'AP50': 14.30159649646293, 'AP75': 3.7232238760916982, 'APs': nan, 'APm': 7.504430419086761, 'APl': nan})])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/499 [00:00<00:15, 32.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating recall @ 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:16<00:00, 30.10it/s]\n",
      "  1%|          | 4/499 [00:00<00:14, 33.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: @0.5: \n",
      "\n",
      " CHAP-PF05257: 0.0, (0, 507)\n",
      "Calculating recall @ 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:16<00:00, 30.05it/s]\n",
      "  1%|          | 4/499 [00:00<00:15, 32.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: @0.8: \n",
      "\n",
      " CHAP-PF05257: 0.0, (0, 507)\n",
      "Calculating recall @ 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:16<00:00, 30.29it/s]\n",
      "  1%|          | 4/499 [00:00<00:14, 33.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: @0.9: \n",
      "\n",
      " CHAP-PF05257: 0.0, (0, 507)\n",
      "Calculating recall @ 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [00:16<00:00, 30.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: @0.99: \n",
      "\n",
      " CHAP-PF05257: 0.0, (0, 507)\n",
      "Dropping 0 sequences which are common with ['CHAP-PF05257']'s sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/11364 [00:00<07:00, 27.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 11364 images from classes ['Amidase_2-PF01510', 'Amidase_3-PF01520'] for open set recognition test...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11364/11364 [06:07<00:00, 30.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions with score >0.99 0 out of 11364: 0.0\n",
      "Number of predictions with score >0.9 0 out of 11364: 0.0\n",
      "Number of predictions with score >0.8 0 out of 11364: 0.0\n",
      "Number of predictions with score >0.7 0 out of 11364: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from functools import partial\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from Bio import SeqIO\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from google.colab.patches import cv2_imshow\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ProjectRoot = Path(__file__).resolve().parent.parent.parent\n",
    "ProjectRoot = Path('/home/satish27may/ProteinDomainDetection')\n",
    "print(f\"ProjectRoot: {str(ProjectRoot)}\")\n",
    "\n",
    "sys.path.append(str(ProjectRoot))\n",
    "\n",
    "def protein_seq2image(item=None, color_map=None):\n",
    "    \"\"\"\n",
    "    Create an image from a sequence\n",
    "    \"\"\"\n",
    "    sequence,img_name, img_h, img_w = item\n",
    "    assert len(sequence)<=img_w, f\"!! Len of sequence({len(sequence)}) should be less than img_w({img_w}), \"\n",
    "    image = np.full((img_h, img_w,3), (500,500,500))\n",
    "    for index in range(len(sequence)):\n",
    "        image[:, index, :] = color_map[sequence[index]]\n",
    "    pil_image = Image.fromarray(image.astype(np.uint8))\n",
    "    assert pil_image.size == (img_w, img_h), f\"{pil_image.size}!=({img_w},{img_h})\"\n",
    "    pil_image.save(img_name)\n",
    "    \n",
    "def protein_seq2image_seqlen(item=None, color_map=None):\n",
    "    \"\"\"\n",
    "    Create an image from a sequence\n",
    "    \"\"\"\n",
    "    sequence,img_name, img_h, img_w = item\n",
    "    img_w = len(sequence)\n",
    "    assert len(sequence)<=img_w, f\"!! Len of sequence({len(sequence)}) should be less than img_w({img_w}), \"\n",
    "    image = np.full((img_h, img_w,3), (500,500,500))\n",
    "    for index in range(len(sequence)):\n",
    "        image[:, index, :] = color_map[sequence[index]]\n",
    "    pil_image = Image.fromarray(image.astype(np.uint8))\n",
    "    assert pil_image.size == (img_w, img_h), f\"{pil_image.size}!=({img_w},{img_h})\"\n",
    "    pil_image.save(img_name)\n",
    "\n",
    "\n",
    "\n",
    "class Data:\n",
    "    \n",
    "    def __init__(self, classes) -> None:\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.data_dir = ProjectRoot/'data'\n",
    "        self.images_dir = ProjectRoot/'data/PfamData/protein_seq_images'\n",
    "        if not self.images_dir.exists():\n",
    "            self.images_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "        self.color_map = {}\n",
    "        index = 0\n",
    "        for amino_acid in string.ascii_uppercase:\n",
    "            self.color_map[amino_acid] = (index+10, index+10, index+10)\n",
    "            index = index+10\n",
    "        \n",
    "    def create_protein_domain_data(self):\n",
    "        records = []\n",
    "        domain_data_records=[]\n",
    "        seq_data_records = []\n",
    "        for class_name in self.classes:\n",
    "            cls_img_dir = self.images_dir/f\"{class_name}\"\n",
    "            cls_img_dir.mkdir(exist_ok=True, parents=True)\n",
    "            super_class, class_id = class_name.split('-')\n",
    "            full_seq_data = self.data_dir/f'PfamData/{super_class}___full_sequence_data/{class_name}___full_sequence_data.fasta'\n",
    "            dom_data = self.data_dir/f'PfamData/{super_class}___full_sequence_data/{class_name}___domain_data.fasta'\n",
    "            # parse sequences of all classes\n",
    "            for record in SeqIO.parse(full_seq_data, 'fasta'):\n",
    "                seq_data_records.append({'Sequence': record.seq._data,\n",
    "                                        'name': record.name,\n",
    "                                        'id': record.id,\n",
    "                                        'Class':class_id,\n",
    "                                        'SeqLen':len(record.seq._data),\n",
    "                                        'SuperClass':super_class}\n",
    "                                      )\n",
    "            \n",
    "            # parse domains of all classes\n",
    "            for record in SeqIO.parse(dom_data, 'fasta'):\n",
    "                domain_data_records.append({'id':record.id.split('/')[0],\n",
    "                                            'dom':record.seq._data,\n",
    "                                            'dom_pos':tuple([int(pos)-1 for  pos in record.id.split('/')[-1].split('-')]),\n",
    "                                            'dom_len':len(record.seq._data)\n",
    "                                            })\n",
    "        seq_data_df = pd.DataFrame(data=seq_data_records)\n",
    "        domain_data_df = pd.DataFrame(data=domain_data_records)\n",
    "        all_data = pd.merge(seq_data_df, domain_data_df,how='inner',on='id')\n",
    "        all_data.drop_duplicates(inplace=True)\n",
    "        \n",
    "        for index, sequence in enumerate(all_data['Sequence'].unique()):\n",
    "            sequence_df = all_data[all_data['Sequence']==sequence]\n",
    "            \n",
    "            records.append({'Sequence':sequence,\n",
    "                            'Class':'||'.join(sequence_df['Class']),\n",
    "                            'SuperClass':'||'.join(sequence_df['SuperClass']),\n",
    "                            'name': '||'.join(sequence_df['name']),\n",
    "                            'SeqLen':sequence_df['SeqLen'].tolist()[0],\n",
    "                            'dom':sequence_df['dom'].tolist(),\n",
    "                            'dom_pos':sequence_df['dom_pos'].tolist(),\n",
    "                            'dom_len':sequence_df['dom_len'].tolist(),\n",
    "                            'img_pth':self.images_dir/f\"img_{index}.png\",\n",
    "                            })\n",
    "        return pd.DataFrame(data=records)\n",
    "    \n",
    "    def create_protein_seq_images(self, data_df, img_h, img_w):\n",
    "        unique_classes = []\n",
    "        for cls_name in data_df['Class'].unique():\n",
    "            cls_list = cls_name.split('||')\n",
    "            if len(cls_list)>1:\n",
    "                unique_classes.extend(cls_list)\n",
    "            else:\n",
    "                if type(cls_list)==list:\n",
    "                    unique_classes.extend(cls_list)\n",
    "                else:\n",
    "                    unique_classes.append(cls_list)\n",
    "        unique_classes = list(set(unique_classes))\n",
    "        print(f\"Generating images of dim {img_h}x{img_w} for classes: {unique_classes}\")\n",
    "        for class_name in self.classes:\n",
    "            if not (self.images_dir/f'{class_name}').exists():\n",
    "                (self.images_dir/f'{class_name}').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        partial_protein_seq2image = partial(protein_seq2image,color_map=self.color_map)\n",
    "        items = [(sequence, img_name, img_h, img_w ) for sequence, img_name in zip(data_df['Sequence'],data_df['img_pth'])]\n",
    "        with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as p:\n",
    "            p.map(partial_protein_seq2image, items)\n",
    "            \n",
    "    def create_protein_seq_len_images(self, data_df, img_h, img_w):\n",
    "        unique_classes = []\n",
    "        for cls_name in data_df['Class'].unique():\n",
    "            cls_list = cls_name.split('||')\n",
    "            if len(cls_list)>1:\n",
    "                unique_classes.extend(cls_list)\n",
    "            else:\n",
    "                unique_classes.append(cls_list)\n",
    "        #unique_classes = list(set(unique_classes))\n",
    "        #print(f\"Generating images of dim {img_h}x{img_w} for classes: {unique_classes}\")\n",
    "        for class_name in self.classes:\n",
    "            if not (self.images_dir/f'{class_name}').exists():\n",
    "                (self.images_dir/f'{class_name}').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        partial_protein_seq2image_seqlen = partial(protein_seq2image_seqlen,color_map=self.color_map)\n",
    "        items = [(sequence, img_name, img_h, img_w ) for sequence, img_name in zip(data_df['Sequence'],data_df['img_pth'])]\n",
    "        with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as p:\n",
    "            p.map(partial_protein_seq2image_seqlen, items)\n",
    "            \n",
    "class Detectron:\n",
    "    \n",
    "    def __init__(self, classes=None, model_dir=None, img_h=None, img_w=None) -> None:\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.model_dir = model_dir\n",
    "        os.system(f\"! trash-put {str(self.model_dir)}\")\n",
    "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cfg = get_cfg()\n",
    "        self.cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"))\n",
    "        self.cfg.DATASETS.TRAIN = (\"train\",)\n",
    "        self.cfg.DATASETS.TEST = (\"valid\",)\n",
    "        self.cfg.INPUT.RANDOM_FLIP = \"vertical\"\n",
    "        self.cfg.TEST.DETECTIONS_PER_IMAGE = 100\n",
    "\n",
    "        self.cfg.INPUT.MIN_SIZE_TRAIN = 800#64\n",
    "        self.cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING = \"choice\"\n",
    "        self.cfg.INPUT.MAX_SIZE_TRAIN = 1330#300\n",
    "        self.cfg.INPUT.MIN_SIZE_TEST = 800#64\n",
    "        self.cfg.INPUT.MAX_SIZE_TEST = 1330#300\n",
    "\n",
    "        self.cfg.TEST.AUG.FLIP = False\n",
    "        self.cfg.DATALOADER.NUM_WORKERS = 8\n",
    "        self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\")  \n",
    "        self.cfg.SOLVER.IMS_PER_BATCH = 8\n",
    "        self.cfg.SOLVER.BASE_LR = 3e-3  \n",
    "        self.cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\n",
    "        # self.cfg.SOLVER.MAX_ITER = 3000\n",
    "        print(f\"Number of classes model is seeing: {len(classes)}\")\n",
    "        self.cfg.MODEL.RETINANET.NUM_CLASSES = len(classes)\n",
    "        self.cfg.MODEL.BACKBONE.FREEZE_AT=3\n",
    "\n",
    "        # exp\n",
    "#         self.cfg.MODEL.RESNETS.NORM = \"BN\"\n",
    "        # self.cfg.MODEL.RETINANET.FOCAL_LOSS_GAMMA =0.5 #didn't work\n",
    "#         self.cfg.MODEL.RETINANET.IOU_THRESHOLDS = [0.4, 0.99]\n",
    "        \n",
    "        self.cfg.OUTPUT_DIR =str(self.model_dir)\n",
    "        os.makedirs(self.cfg.OUTPUT_DIR, exist_ok=True)\n",
    "        \n",
    "    def get_value_counts(self, df, col, sep):\n",
    "        values_container = []\n",
    "        for value in df[col]:\n",
    "            value_list = value.split(sep)\n",
    "            values_container.extend(value_list)\n",
    "        return Counter(values_container)\n",
    "            \n",
    "        \n",
    "    @staticmethod\n",
    "    def add_gaussian_noise(img_pth,dim, index)->str:\n",
    "        seq = iaa.Sequential([iaa.SaltAndPepper(0.05)])\n",
    "        img_arrs = np.zeros((2,dim[0], dim[1], 3))\n",
    "        # print(f\"img_arrsshape: {img_arrs.shape} \")\n",
    "        # print(f\"img_pth:{img_pth}\")\n",
    "        # print(f\"img shape:{np.array(Image.open(img_pth)).shape}\")\n",
    "        img_arrs[0,:,:,:] = np.array(Image.open(img_pth))\n",
    "        images_aug = seq(images=img_arrs)[0]\n",
    "        aug_img_pth = img_pth.parent/f'{img_pth.stem}_gaussian_noise_{index}.png'\n",
    "        Image.fromarray(images_aug.astype(np.uint8)).save(aug_img_pth)\n",
    "        return str(aug_img_pth)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def add_cutout(img_pth, dim, index)->str:\n",
    "        seq = iaa.Sequential([iaa.Cutout(nb_iterations=(10, 20), size=0.05, squared=False)])\n",
    "        img_arrs = np.zeros((2,dim[0], dim[1], 3))\n",
    "        img_arrs[0,:,:,:] = np.array(Image.open(img_pth))\n",
    "        images_aug = seq(images=img_arrs)[0]\n",
    "        aug_img_pth = img_pth.parent/f'{img_pth.stem}_cutout_{index}.png'\n",
    "        Image.fromarray(images_aug.astype(np.uint8)).save(aug_img_pth)\n",
    "        return str(aug_img_pth)\n",
    "    \n",
    "    def augment_data(self,data_df):\n",
    "        class_freq = dict(self.get_value_counts(data_df, 'Class', '||'))\n",
    "        max_value = max(class_freq.values())\n",
    "        dim = (img_h, img_w)\n",
    "        print('Augmenting train data.................')\n",
    "        print(f'Max samples: {max_value}')\n",
    "        classes = [cls_name.split('-')[1] for cls_name in self.classes]\n",
    "        new_rows = []\n",
    "        for cls_name in classes:\n",
    "            class_df = data_df[data_df['Class'].str.contains(cls_name)]\n",
    "            all_rows = [row for _, row in class_df.iterrows()]\n",
    "            num_cls_samples = class_freq[cls_name]\n",
    "            num_augs = max_value - num_cls_samples\n",
    "            print(f\"Creating {num_augs} augs for {cls_name}\")\n",
    "            for index in range(int(round(num_augs/2))):\n",
    "                row = random.choice(all_rows)\n",
    "                img_pth = Path(row['img_pth'])\n",
    "                gaussian_noise = row.copy()\n",
    "                gaussian_noise['img_pth'] = self.add_gaussian_noise(img_pth,dim, index)\n",
    "                cutout = row.copy()\n",
    "                cutout['img_pth'] = self.add_cutout(img_pth,dim, index)\n",
    "                new_rows.extend([cutout, gaussian_noise])\n",
    "#         print(data_df.columns)\n",
    "#         print(new_rows[:10])\n",
    "        aug_data = pd.DataFrame(data=new_rows)\n",
    "#         print(data_df.head())\n",
    "#         print(aug_data.head())\n",
    "        data_df = pd.concat([data_df, ],axis='rows')\n",
    "        data_df.reset_index(drop=True, inplace=True)\n",
    "        return data_df.sample(frac=1)\n",
    "    def create_train_valid_data(self, data):\n",
    "        train_dfs,valid_dfs = [],[],\n",
    "        for class_name in self.classes:\n",
    "            class_id = class_name.split('-')[-1]\n",
    "            class_df = data[data['Class'].str.contains(class_id)].sample(frac=1)\n",
    "            num_samples = class_df.shape[0]\n",
    "            num_train_samples = int(round(num_samples*0.7))\n",
    "            train_dfs.append(class_df.iloc[:num_train_samples,:])\n",
    "            valid_dfs.append(class_df.iloc[num_train_samples:,:])\n",
    "        train_data = pd.concat(train_dfs,axis='rows').sample(frac=1)\n",
    "#         train_data = self.augment_data(train_data)### augment train data!!!!!!!!!!!!!!!!!!\n",
    "        valid_data = pd.concat(valid_dfs,axis='rows').sample(frac=1)\n",
    "        print(f'Train data dist:\\n')\n",
    "        print(self.get_value_counts(train_data,'Class','||'))\n",
    "        print(f'Valid data dist:\\n')\n",
    "        print(self.get_value_counts(valid_data,'Class','||'))\n",
    "        #assert len(set(list(train_data['id'])).intersection(set(list(valid_data['id']))))==0, f\"There is a data leak between train and valid\"\n",
    "        return train_data, valid_data\n",
    "        \n",
    "    def register_custom_data(self, data, mode, img_h, img_w):\n",
    "        print(f\"Registring {mode} data.......\")\n",
    "        print(f\"Classes selected: {self.classes}\")\n",
    "        # print(f\"SuperClasses selected: {data['SuperClass'].unique()}\")\n",
    "        data = data.reset_index(drop=True)\n",
    "        self.C2I = {class_name:index for index, class_name in enumerate(self.classes)}\n",
    "        dicts_list = []\n",
    "        data = data.reset_index(drop=True)\n",
    "        for index in tqdm.tqdm(range(data.shape[0])):\n",
    "            class_list = data['Class'][index].split('||')\n",
    "            super_class_list = data['SuperClass'][index].split('||')\n",
    "            pil_img = Image.open(data['img_pth'][index])\n",
    "            img_w, img_h = pil_img.size\n",
    "            dom_pos_list = data['dom_pos'][index]\n",
    "            if len(dom_pos_list)>1:\n",
    "                annts = []\n",
    "                for dom_index in range(len(dom_pos_list)):\n",
    "                    x1,x2 = dom_pos_list[dom_index]\n",
    "                    annts.append({'bbox':[x1, 0, x2, img_h],\n",
    "                                            'bbox_mode':BoxMode.XYXY_ABS,\n",
    "                                            'category_id':  self.C2I[f\"{super_class_list[dom_index]}-{class_list[dom_index]}\"],\n",
    "                                            })\n",
    "            elif len(dom_pos_list)==1:\n",
    "                x1,x2 = dom_pos_list[0]\n",
    "                annts = [{'bbox':[x1, 0, x2, img_h],\n",
    "                                            'bbox_mode':BoxMode.XYXY_ABS,\n",
    "                                            'category_id':  self.C2I[f\"{super_class_list[0]}-{class_list[0]}\"],\n",
    "                                            }]\n",
    "                \n",
    "                    \n",
    "            dicts_list.append({'file_name':data['img_pth'][index],\n",
    "                            'height':img_h,\n",
    "                            'width': img_w,\n",
    "                            'image_id': index,\n",
    "                            'annotations':annts,\n",
    "                            })\n",
    "        def get_data():\n",
    "            return dicts_list\n",
    "        DatasetCatalog.register(mode, get_data)\n",
    "        MetadataCatalog.get(mode).set(thing_classes = self.classes)\n",
    "        \n",
    "    def train(self):\n",
    "        trainer = DefaultTrainer(self.cfg) \n",
    "        trainer.resume_or_load(resume=False)\n",
    "        trainer.train() \n",
    "        return trainer\n",
    "\n",
    "    def evaluate(self, trainer):\n",
    "        # evaluate model\n",
    "        evaluator = COCOEvaluator(\"valid\", (\"bbox\",), False, output_dir=self.cfg.OUTPUT_DIR )\n",
    "        val_loader = build_detection_test_loader(self.cfg, \"valid\")\n",
    "        print(trainer.test(self.cfg, trainer.model, evaluator))\n",
    "        \n",
    "        \n",
    "    def inference(self):\n",
    "        self.cfg.MODEL.WEIGHTS = os.path.join(self.cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "        self.cfg.MODEL.RETINANET.SCORE_THRESH_TEST =  0.5  # set a custom testing threshold\n",
    "        predictor = DefaultPredictor(self.cfg)\n",
    "        self.I2C = {value:key for key, value in self.C2I.items()}\n",
    "        theta=0.9\n",
    "        def calculate_recall(theta):\n",
    "            print(f\"Calculating recall @ {theta}\")\n",
    "            class_recall_map = {class_name:{'num_bboxes':0, 'pred_bboxes':0} for class_name in self.classes}\n",
    "            num_bboxes = 0\n",
    "            pred_bboxes = 0\n",
    "            for d in tqdm.tqdm(DatasetCatalog.get(\"valid\")):\n",
    "                im = cv2.imread(str(d[\"file_name\"]))\n",
    "                for antn_index in range(len(d['annotations'])):\n",
    "                    class_recall_map[self.I2C[d['annotations'][antn_index]['category_id']]]['num_bboxes']+=1\n",
    "                \n",
    "                outputs = predictor(im)\n",
    "\n",
    "                for index,score in enumerate(outputs['instances'].get_fields()['scores']):\n",
    "                    if score.cpu().numpy().item() >= theta :\n",
    "                        for anntn in d['annotations']:\n",
    "                            if anntn['category_id']==outputs['instances'].get_fields()['pred_classes'][index].cpu().numpy().item():\n",
    "                                class_recall_map[self.I2C[anntn['category_id']]]['pred_bboxes']+=1\n",
    "\n",
    "            print(f\"Recall: @{theta}: \\n\")\n",
    "            for cls_name, value in class_recall_map.items():\n",
    "                print(f\" {cls_name}: {value['pred_bboxes']/value['num_bboxes']}, {value['pred_bboxes'], value['num_bboxes']}\")\n",
    "        try:\n",
    "            calculate_recall(0.5)\n",
    "            calculate_recall(0.8)\n",
    "            calculate_recall(0.9)\n",
    "            calculate_recall(0.99)\n",
    "        except:\n",
    "            pass\n",
    "        # for d in random.sample(dataset_dicts, 3):    \n",
    "        #     im = cv2.imread(d[\"file_name\"])\n",
    "        #     outputs = predictor(im)  \n",
    "        #     v = Visualizer(im[:, :, ::-1],\n",
    "        #                 metadata=MetadataCatalog.get(\"train\").set(thing_classes = self.classes), \n",
    "        #                 scale=1.5, \n",
    "        #                 instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "        #     )\n",
    "        #     out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        #     cv2_imshow(out.get_image()[:, :, ::-1])\n",
    "        return predictor\n",
    "            \n",
    "    def test_for_open_set( self, class_df, classes, img_h, img_w, predictor):                                              \n",
    "        data_handler = Data(classes)                                                                                       \n",
    "        protein_domain_data = data_handler.create_protein_domain_data()                                                    \n",
    "        num_rows = protein_domain_data.shape[0]                                                                            \n",
    "        protein_domain_data = protein_domain_data[~protein_domain_data['Sequence'].isin(class_df['Sequence'])]             \n",
    "        print(f\"Dropping {abs(num_rows - protein_domain_data.shape[0])} sequences which are common with {self.classes}'s sequences\")\n",
    "        protein_domain_data = protein_domain_data[protein_domain_data['SeqLen']<img_w]                        \n",
    "#         data_handler.create_protein_seq_images(protein_domain_data, img_h, img_w) \n",
    "        data_handler.create_protein_seq_len_images(protein_domain_data, img_h, img_w)                           \n",
    "        all_imgs = protein_domain_data['img_pth'].tolist()                                                    \n",
    "        print(f\"Using {len(all_imgs)} images from classes {classes} for open set recognition test...........\")\n",
    "        count_0_99 = 0                                                                             \n",
    "        count_0_9 = 0                                                                              \n",
    "        count_0_8 = 0                                                                                                                                \n",
    "        count_0_7 = 0                                                                                                                                \n",
    "        for img_pth in tqdm.tqdm(all_imgs):                                                                                                          \n",
    "            im = cv2.imread(str(img_pth))                                                                                                            \n",
    "            outputs = predictor(im)                                                                                                                  \n",
    "            # v = Visualizer(im[:, :, ::-1],                                                                                                         \n",
    "            #             metadata=MetadataCatalog.get(\"valid\").set(thing_classes = self.classes),                                                   \n",
    "            #             scale=0.5,                                                                                                                 \n",
    "            #             instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models                                                                                                                                     \n",
    "            # )                                                                                                                              \n",
    "            # out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))                                                              \n",
    "            # cv2_imshow(out.get_image()[:, :, ::-1])                                                                                        \n",
    "            try:                                                                                                                             \n",
    "                instance = outputs['instances']                                                                                              \n",
    "                if max(instance.get_fields()['scores']).cpu().numpy() >= 0.99:                                                               \n",
    "                    count_0_99+=1                                                                                                            \n",
    "                elif max(instance.get_fields()['scores']).cpu().numpy() >= 0.9 and max(instance.get_fields()['scores']).cpu().numpy() < 0.99:\n",
    "                    count_0_9+=1                                                                                                            \n",
    "                elif max(instance.get_fields()['scores']).cpu().numpy() >= 0.8 and max(instance.get_fields()['scores']).cpu().numpy() < 0.9:\n",
    "                    count_0_8+=1                                                                                                            \n",
    "                elif max(instance.get_fields()['scores']).cpu().numpy() >= 0.7 and max(instance.get_fields()['scores'].cpu().numpy()) < 0.8:\n",
    "                    count_0_7+=1                                                                                        \n",
    "            except Exception as e:                                                                                      \n",
    "                #print(max(instance.get_fields()['scores']).cpu().numpy())                                              \n",
    "                #print(e)                                                                                               \n",
    "                pass                                                                                                    \n",
    "                                                                                                                        \n",
    "        print(f\"Number of predictions with score >0.99 {count_0_99} out of {len(all_imgs)}: {count_0_99/len(all_imgs)}\")\n",
    "        print(f\"Number of predictions with score >0.9 {count_0_9} out of {len(all_imgs)}: {count_0_9/len(all_imgs)}\")\n",
    "        print(f\"Number of predictions with score >0.8 {count_0_8} out of {len(all_imgs)}: {count_0_8/len(all_imgs)}\")\n",
    "        print(f\"Number of predictions with score >0.7 {count_0_7} out of {len(all_imgs)}: {count_0_7/len(all_imgs)}\")\n",
    "      \n",
    "      \n",
    "# if __name__ == \"__main__\":\n",
    "all_classes = ['Amidase_2-PF01510',\n",
    "'Amidase_3-PF01520',]\n",
    "classes=['CHAP-PF05257']#, 'SH3_3-PF08239', 'SH3_4-PF06347']# 'SH3_4-PF06347','SH3_5-PF08460']\n",
    "        # 'SH3_3-PF08239',\n",
    "        # 'SH3_5-PF08460',\n",
    "        # 'LysM-PF01476']\n",
    "img_h, img_w = 64, 300 # padding is also controlled by this img_w\n",
    "seq_len =300   \n",
    "seq_buckets = (0, img_w) \n",
    "\n",
    "data_block = Data(classes)\n",
    "protein_data = data_block.create_protein_domain_data()\n",
    "protein_data = protein_data[protein_data['SeqLen']<seq_len]\n",
    "\n",
    "# Select majority domains from each class based on dom len dist\n",
    "# CHAP>>60-120 SH3_4>>50-63 SH3_3>>45-75\n",
    "chap_indexs = []\n",
    "chap_data = protein_data[protein_data['Class'].str.contains('PF05257')]\n",
    "for index, row in chap_data.iterrows():\n",
    "    flag=False\n",
    "    for dom_len in row['dom_len']:\n",
    "        if dom_len >= 70 and dom_len <=102:\n",
    "            flag=True\n",
    "        else:\n",
    "            flag=False\n",
    "    if flag:\n",
    "        chap_indexs.append(index)\n",
    "        \n",
    "# sh3_4_indexs = []\n",
    "# sh3_4_data = protein_data[protein_data['Class'].str.contains('PF06347')]\n",
    "# for index, row in sh3_4_data.iterrows():\n",
    "#     flag=False\n",
    "#     for dom_len in row['dom_len']:\n",
    "#         if dom_len >=52 and dom_len <=58:\n",
    "#             flag=True\n",
    "#         else:\n",
    "#             flag=False\n",
    "#     if flag:\n",
    "#         sh3_4_indexs.append(index)\n",
    "            \n",
    "# sh3_3_indexs = []\n",
    "# sh3_3_data = protein_data[protein_data['Class'].str.contains('PF08239')]\n",
    "# for index, row in sh3_3_data.iterrows():\n",
    "#     flag=False\n",
    "#     for dom_len in row['dom_len']:\n",
    "#         if dom_len >=50 and dom_len <=60:\n",
    "#             flag=True\n",
    "#         else:\n",
    "#             flag=False\n",
    "#     if flag:\n",
    "#         sh3_3_indexs.append(index)\n",
    "\n",
    "num_rows = protein_data.shape[0]\n",
    "protein_data = protein_data[protein_data.index.isin(chap_indexs)]#+sh3_3_indexs+sh3_4_indexs)]\n",
    "print(f\"Number of records dropeed after selecting major domains based on len: {num_rows - protein_data.shape[0]}\")\n",
    "            \n",
    "# data_block.create_protein_seq_images(protein_data, img_h, img_w)\n",
    "data_block.create_protein_seq_len_images(protein_data, img_h, img_w)\n",
    "\n",
    "model_block = Detectron(classes=classes, model_dir=ProjectRoot/f\"models/{'_'.join(classes)}_{seq_buckets[0]}_{seq_buckets[1]}\",img_h=img_h,img_w=img_w)\n",
    "model_block.cfg.SOLVER.MAX_ITER = 30\n",
    "train_data, valid_data = model_block.create_train_valid_data(protein_data)\n",
    "model_block.register_custom_data(train_data, 'train', img_h, img_w)\n",
    "model_block.register_custom_data(valid_data, 'valid', img_h, img_w)\n",
    "trainer = model_block.train()\n",
    "model_block.evaluate(trainer)\n",
    "predictor = model_block.inference()\n",
    "#class_index = all_classes.index(classes[0])\n",
    "#del all_classes[class_index]\n",
    "model_block.test_for_open_set(protein_data, all_classes, img_h, img_w, predictor)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Less than 10k training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProjectRoot: /home/satish27may/ProteinDomainDetection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:01<00:00,  7.88it/s]\n",
      "31947it [03:04, 169.89it/s]"
     ]
    }
   ],
   "source": [
    "# from functools import partial\n",
    "import multiprocessing\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from Bio import SeqIO\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from ast import literal_eval\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from google.colab.patches import cv2_imshow\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ProjectRoot = Path(__file__).resolve().parent.parent.parent\n",
    "ProjectRoot = Path('/home/satish27may/ProteinDomainDetection')\n",
    "print(f\"ProjectRoot: {str(ProjectRoot)}\")\n",
    "\n",
    "sys.path.append(str(ProjectRoot))\n",
    "\n",
    "def protein_seq2image(item=None, color_map=None):\n",
    "    \"\"\"\n",
    "    Create an image from a sequence\n",
    "    \"\"\"\n",
    "    sequence,img_name, img_h, img_w = item\n",
    "    assert len(sequence)<=img_w, f\"!! Len of sequence({len(sequence)}) should be less than img_w({img_w}), \"\n",
    "    image = np.full((img_h, img_w,3), (500,500,500))\n",
    "    for index in range(len(sequence)):\n",
    "        image[:, index, :] = color_map[sequence[index]]\n",
    "    pil_image = Image.fromarray(image.astype(np.uint8))\n",
    "    assert pil_image.size == (img_w, img_h), f\"{pil_image.size}!=({img_w},{img_h})\"\n",
    "    pil_image.save(img_name)\n",
    "    \n",
    "def protein_seq2image_seqlen(item=None, color_map=None):\n",
    "    \"\"\"\n",
    "    Create an image from a sequence\n",
    "    \"\"\"\n",
    "    sequence,img_name, img_h, img_w = item\n",
    "    img_w = len(sequence)\n",
    "    assert len(sequence)<=img_w, f\"!! Len of sequence({len(sequence)}) should be less than img_w({img_w}), \"\n",
    "    image = np.full((img_h, img_w,3), (500,500,500))\n",
    "    for index in range(len(sequence)):\n",
    "        image[:, index, :] = color_map[sequence[index]]\n",
    "    pil_image = Image.fromarray(image.astype(np.uint8))\n",
    "    assert pil_image.size == (img_w, img_h), f\"{pil_image.size}!=({img_w},{img_h})\"\n",
    "    pil_image.save(img_name)\n",
    "\n",
    "\n",
    "\n",
    "class Data:\n",
    "    \n",
    "    def __init__(self, classes) -> None:\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.data_dir = ProjectRoot/'data'\n",
    "        self.images_dir = ProjectRoot/'data/PfamData/protein_seq_images'\n",
    "        if not self.images_dir.exists():\n",
    "            self.images_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "        self.color_map = {}\n",
    "        index = 0\n",
    "        for amino_acid in string.ascii_uppercase:\n",
    "            self.color_map[amino_acid] = (index+10, index+10, index+10)\n",
    "            index = index+10\n",
    "        \n",
    "    def create_protein_domain_data(self):\n",
    "        records = []\n",
    "        domain_data_records=[]\n",
    "        seq_data_records = []\n",
    "        for class_name in tqdm.tqdm(self.classes):\n",
    "            cls_img_dir = self.images_dir/f\"{class_name}\"\n",
    "            cls_img_dir.mkdir(exist_ok=True, parents=True)\n",
    "            super_class, class_id = class_name.split('-')\n",
    "            full_seq_data = self.data_dir/f'PfamData/{super_class}___full_sequence_data/{class_name}___full_sequence_data.fasta'\n",
    "            dom_data = self.data_dir/f'PfamData/{super_class}___full_sequence_data/{class_name}___domain_data.fasta'\n",
    "            # parse sequences of all classes\n",
    "            for record in SeqIO.parse(full_seq_data, 'fasta'):\n",
    "                seq_data_records.append({'Sequence': record.seq._data,\n",
    "                                        'name': record.name,\n",
    "                                        'id': record.id,\n",
    "                                        'Class':class_id,\n",
    "                                        'SeqLen':len(record.seq._data),\n",
    "                                        'SuperClass':super_class}\n",
    "                                      )\n",
    "            \n",
    "            # parse domains of all classes\n",
    "            for record in SeqIO.parse(dom_data, 'fasta'):\n",
    "                domain_data_records.append({'id':record.id.split('/')[0],\n",
    "                                            'dom':record.seq._data,\n",
    "                                            'dom_pos':tuple([int(pos)-1 for  pos in record.id.split('/')[-1].split('-')]),\n",
    "                                            'dom_len':len(record.seq._data)\n",
    "                                            })\n",
    "        seq_data_df = pd.DataFrame(data=seq_data_records)\n",
    "        domain_data_df = pd.DataFrame(data=domain_data_records)\n",
    "        all_data = pd.merge(seq_data_df, domain_data_df,how='inner',on='id')\n",
    "        all_data.drop_duplicates(inplace=True)\n",
    "        \n",
    "        for index, sequence in tqdm.tqdm(enumerate(all_data['Sequence'].unique())):\n",
    "            sequence_df = all_data[all_data['Sequence']==sequence]\n",
    "            \n",
    "            classes_in_record = '-'.join(list(set(sequence_df['Class'])))\n",
    "            records.append({'Sequence':sequence,\n",
    "                            'Class':'||'.join(sequence_df['Class']),\n",
    "                            'SuperClass':'||'.join(sequence_df['SuperClass']),\n",
    "                            'name': '||'.join(sequence_df['name']),\n",
    "                            'SeqLen':sequence_df['SeqLen'].tolist()[0],\n",
    "                            'dom':sequence_df['dom'].tolist(),\n",
    "                            'dom_pos':sequence_df['dom_pos'].tolist(),\n",
    "                            'dom_len':sequence_df['dom_len'].tolist(),\n",
    "                            'img_pth':self.images_dir/f\"img_{index}.png\",\n",
    "                            })\n",
    "        return pd.DataFrame(data=records)\n",
    "    \n",
    "    def create_protein_seq_images(self, data_df, img_h, img_w):\n",
    "        unique_classes = []\n",
    "        for cls_name in data_df['Class'].unique():\n",
    "            cls_list = cls_name.split('||')\n",
    "            if len(cls_list)>1:\n",
    "                unique_classes.extend(cls_list)\n",
    "            else:\n",
    "                if type(cls_list)==list:\n",
    "                    unique_classes.extend(cls_list)\n",
    "                else:\n",
    "                    unique_classes.append(cls_list)\n",
    "        unique_classes = list(set(unique_classes))\n",
    "        print(f\"Generating images of dim {img_h}x{img_w} for classes: {unique_classes}\")\n",
    "        for class_name in self.classes:\n",
    "            if not (self.images_dir/f'{class_name}').exists():\n",
    "                (self.images_dir/f'{class_name}').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        partial_protein_seq2image = partial(protein_seq2image,color_map=self.color_map)\n",
    "        print('Creating items')\n",
    "        items = [(sequence, img_name, img_h, img_w ) for sequence, img_name in zip(data_df['Sequence'],data_df['img_pth'])]\n",
    "        print('Creating images with multi processing')\n",
    "        with multiprocessing.Pool(processes=multiprocessing.cpu_count()-1) as p:\n",
    "            p.map(partial_protein_seq2image, items)\n",
    "#         for item in tqdm.tqdm(items):\n",
    "#             partial_protein_seq2image(item)\n",
    "            \n",
    "    def create_protein_seq_len_images(self, data_df, img_h, img_w):\n",
    "        unique_classes = []\n",
    "        for cls_name in data_df['Class'].unique():\n",
    "            cls_list = cls_name.split('||')\n",
    "            if len(cls_list)>1:\n",
    "                unique_classes.extend(cls_list)\n",
    "            else:\n",
    "                unique_classes.append(cls_list)\n",
    "        #unique_classes = list(set(unique_classes))\n",
    "        #print(f\"Generating images of dim {img_h}x{img_w} for classes: {unique_classes}\")\n",
    "        for class_name in self.classes:\n",
    "            if not (self.images_dir/f'{class_name}').exists():\n",
    "                (self.images_dir/f'{class_name}').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        partial_protein_seq2image_seqlen = partial(protein_seq2image_seqlen,color_map=self.color_map)\n",
    "        items = [(sequence, img_name, img_h, img_w ) for sequence, img_name in zip(data_df['Sequence'],data_df['img_pth'])]\n",
    "        with multiprocessing.Pool(processes=12) as p:\n",
    "            p.map(partial_protein_seq2image_seqlen, items)\n",
    "            \n",
    "    def filter_data(self, data_df, class_id, min_dom_len, max_dom_len):\n",
    "        \"\"\"\n",
    "        Function to filter class data based on domain len filters\n",
    "        \"\"\"\n",
    "        filtered_indexes = []\n",
    "        class_df = data_df[data_df['Class'].str.contains(class_id)]\n",
    "        for index, row in class_df.iterrows():\n",
    "            flag=False\n",
    "            for dom_len in row['dom_len']:\n",
    "                if dom_len >= min_dom_len and  dom_len <=max_dom_len:\n",
    "                    flag=True\n",
    "                else:\n",
    "                    flag=False\n",
    "                    break\n",
    "            if flag:\n",
    "                filtered_indexes.append(index)\n",
    "        return filtered_indexes\n",
    "            \n",
    "class Detectron:\n",
    "    \n",
    "    def __init__(self, classes=None, model_dir=None, img_h=None, img_w=None) -> None:\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.model_dir = model_dir\n",
    "        os.system(f\"! rm -r {str(self.model_dir)}\")\n",
    "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.cfg = get_cfg()\n",
    "        self.cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"))\n",
    "        self.cfg.DATASETS.TRAIN = (\"train\",)\n",
    "        self.cfg.DATASETS.TEST = (\"valid\",)\n",
    "        self.cfg.INPUT.RANDOM_FLIP = \"vertical\"\n",
    "        self.cfg.TEST.DETECTIONS_PER_IMAGE = 100\n",
    "\n",
    "        self.cfg.INPUT.MIN_SIZE_TRAIN = 800#64\n",
    "        self.cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING = \"choice\"\n",
    "        self.cfg.INPUT.MAX_SIZE_TRAIN = 1330#300\n",
    "        self.cfg.INPUT.MIN_SIZE_TEST = 800#64\n",
    "        self.cfg.INPUT.MAX_SIZE_TEST = 1330#300\n",
    "\n",
    "        self.cfg.TEST.AUG.FLIP = False\n",
    "        self.cfg.DATALOADER.NUM_WORKERS = 8\n",
    "        self.cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\")  \n",
    "        self.cfg.SOLVER.IMS_PER_BATCH = 32\n",
    "        self.cfg.SOLVER.BASE_LR = 3e-3  \n",
    "        self.cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\n",
    "        # self.cfg.SOLVER.MAX_ITER = 3000\n",
    "        print(f\"Number of classes model is seeing: {len(classes)}\")\n",
    "        self.cfg.MODEL.RETINANET.NUM_CLASSES = len(classes)\n",
    "        self.cfg.MODEL.BACKBONE.FREEZE_AT=2\n",
    "\n",
    "        # exp\n",
    "#         self.cfg.MODEL.RESNETS.NORM = \"BN\"\n",
    "        # self.cfg.MODEL.RETINANET.FOCAL_LOSS_GAMMA =0.5 #didn't work\n",
    "#         self.cfg.MODEL.RETINANET.IOU_THRESHOLDS = [0.4, 0.99]\n",
    "        \n",
    "        self.cfg.OUTPUT_DIR =str(self.model_dir)\n",
    "        os.makedirs(self.cfg.OUTPUT_DIR, exist_ok=True)\n",
    "        \n",
    "    def get_value_counts(self, df, col, sep):\n",
    "        values_container = []\n",
    "        for value in df[col]:\n",
    "            value_list = value.split(sep)\n",
    "            values_container.extend(value_list)\n",
    "        return Counter(values_container)\n",
    "            \n",
    "        \n",
    "    @staticmethod\n",
    "    def add_gaussian_noise(img_pth,dim, index)->str:\n",
    "        seq = iaa.Sequential([iaa.SaltAndPepper(0.05)])\n",
    "        img_arrs = np.zeros((2,dim[0], dim[1], 3))\n",
    "        # print(f\"img_arrsshape: {img_arrs.shape} \")\n",
    "        # print(f\"img_pth:{img_pth}\")\n",
    "        # print(f\"img shape:{np.array(Image.open(img_pth)).shape}\")\n",
    "        img_arrs[0,:,:,:] = np.array(Image.open(img_pth))\n",
    "        images_aug = seq(images=img_arrs)[0]\n",
    "        aug_img_pth = img_pth.parent/f'{img_pth.stem}_gaussian_noise_{index}.png'\n",
    "        Image.fromarray(images_aug.astype(np.uint8)).save(aug_img_pth)\n",
    "        return str(aug_img_pth)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def add_cutout(img_pth, dim, index)->str:\n",
    "        seq = iaa.Sequential([iaa.Cutout(nb_iterations=(10, 20), size=0.05, squared=False)])\n",
    "        img_arrs = np.zeros((2,dim[0], dim[1], 3))\n",
    "        img_arrs[0,:,:,:] = np.array(Image.open(img_pth))\n",
    "        images_aug = seq(images=img_arrs)[0]\n",
    "        aug_img_pth = img_pth.parent/f'{img_pth.stem}_cutout_{index}.png'\n",
    "        Image.fromarray(images_aug.astype(np.uint8)).save(aug_img_pth)\n",
    "        return str(aug_img_pth)\n",
    "    \n",
    "    def augment_data(self,data_df):\n",
    "        class_freq = dict(self.get_value_counts(data_df, 'Class', '||'))\n",
    "        max_value = max(class_freq.values())\n",
    "        dim = (img_h, img_w)\n",
    "        print('Augmenting train data.................')\n",
    "        print(f'Max samples: {max_value}')\n",
    "        classes = [cls_name.split('-')[1] for cls_name in self.classes]\n",
    "        new_rows = []\n",
    "        for cls_name in classes:\n",
    "            class_df = data_df[data_df['Class'].str.contains(cls_name)]\n",
    "            all_rows = [row for _, row in class_df.iterrows()]\n",
    "            num_cls_samples = class_freq[cls_name]\n",
    "            num_augs = max_value - num_cls_samples\n",
    "            print(f\"Creating {num_augs} augs for {cls_name}\")\n",
    "            for index in range(int(round(num_augs/2))):\n",
    "                row = random.choice(all_rows)\n",
    "                img_pth = Path(row['img_pth'])\n",
    "                gaussian_noise = row.copy()\n",
    "                gaussian_noise['img_pth'] = self.add_gaussian_noise(img_pth,dim, index)\n",
    "                cutout = row.copy()\n",
    "                cutout['img_pth'] = self.add_cutout(img_pth,dim, index)\n",
    "                new_rows.extend([cutout, gaussian_noise])\n",
    "#         print(data_df.columns)\n",
    "#         print(new_rows[:10])\n",
    "        aug_data = pd.DataFrame(data=new_rows)\n",
    "#         print(data_df.head())\n",
    "#         print(aug_data.head())\n",
    "        data_df = pd.concat([data_df, aug_data],axis='rows')\n",
    "        data_df.reset_index(drop=True, inplace=True)\n",
    "        return data_df.sample(frac=1)\n",
    "    def create_train_valid_data(self, data):\n",
    "        print('Creaing train and valid data')\n",
    "        train_dfs,valid_dfs = [],[],\n",
    "        for class_name in tqdm.tqdm(self.classes):\n",
    "            class_id = class_name.split('-')[-1]\n",
    "            class_df = data[data['Class'].str.contains(class_id)].sample(frac=1)\n",
    "            num_samples = class_df.shape[0]\n",
    "            num_train_samples = int(round(num_samples*0.7))\n",
    "            train_dfs.append(class_df.iloc[:num_train_samples,:])\n",
    "            valid_dfs.append(class_df.iloc[num_train_samples:,:])\n",
    "        train_data = pd.concat(train_dfs,axis='rows').sample(frac=1)\n",
    "        train_data = self.augment_data(train_data)### augment train data!!!!!!!!!!!!!!!!!!\n",
    "        valid_data = pd.concat(valid_dfs,axis='rows').sample(frac=1)\n",
    "        print(f'Train data dist:\\n')\n",
    "        print(self.get_value_counts(train_data,'Class','||'))\n",
    "        print(f'Valid data dist:\\n')\n",
    "        print(self.get_value_counts(valid_data,'Class','||'))\n",
    "        #assert len(set(list(train_data['id'])).intersection(set(list(valid_data['id']))))==0, f\"There is a data leak between train and valid\"\n",
    "        return train_data, valid_data\n",
    "        \n",
    "    def register_custom_data(self, data, mode, img_h, img_w):\n",
    "        print(f\"Registring {mode} data.......\")\n",
    "        print(f\"Classes selected: {self.classes}\")\n",
    "        # print(f\"SuperClasses selected: {data['SuperClass'].unique()}\")\n",
    "        data = data.reset_index(drop=True)\n",
    "        self.C2I = {class_name:index for index, class_name in enumerate(self.classes)}\n",
    "        dicts_list = []\n",
    "        data = data.reset_index(drop=True)\n",
    "        for index in tqdm.tqdm(range(data.shape[0])):\n",
    "            class_list = data['Class'][index].split('||')\n",
    "            super_class_list = data['SuperClass'][index].split('||')\n",
    "            pil_img = Image.open(data['img_pth'][index])\n",
    "            img_w, img_h = pil_img.size\n",
    "            dom_pos_list = data['dom_pos'][index]\n",
    "            if len(dom_pos_list)>1:\n",
    "                annts = []\n",
    "                for dom_index in range(len(dom_pos_list)):\n",
    "                    x1,x2 = dom_pos_list[dom_index]\n",
    "                    annts.append({'bbox':[x1, 0, x2, img_h],\n",
    "                                            'bbox_mode':BoxMode.XYXY_ABS,\n",
    "                                            'category_id':  self.C2I[f\"{super_class_list[dom_index]}-{class_list[dom_index]}\"],\n",
    "                                            })\n",
    "            elif len(dom_pos_list)==1:\n",
    "                x1,x2 = dom_pos_list[0]\n",
    "                annts = [{'bbox':[x1, 0, x2, img_h],\n",
    "                                            'bbox_mode':BoxMode.XYXY_ABS,\n",
    "                                            'category_id':  self.C2I[f\"{super_class_list[0]}-{class_list[0]}\"],\n",
    "                                            }]\n",
    "                \n",
    "                    \n",
    "            dicts_list.append({'file_name':data['img_pth'][index],\n",
    "                            'height':img_h,\n",
    "                            'width': img_w,\n",
    "                            'image_id': index,\n",
    "                            'annotations':annts,\n",
    "                            })\n",
    "        def get_data():\n",
    "            return dicts_list\n",
    "        DatasetCatalog.register(mode, get_data)\n",
    "        MetadataCatalog.get(mode).set(thing_classes = self.classes)\n",
    "        \n",
    "    def train(self):\n",
    "        trainer = DefaultTrainer(self.cfg) \n",
    "#         trainer.resume_or_load(resume=False)\n",
    "        trainer.train() \n",
    "        return trainer\n",
    "\n",
    "    def evaluate(self, trainer):\n",
    "        # evaluate model\n",
    "        evaluator = COCOEvaluator(\"valid\", (\"bbox\",), False, output_dir=self.cfg.OUTPUT_DIR )\n",
    "        val_loader = build_detection_test_loader(self.cfg, \"valid\")\n",
    "        print(trainer.test(self.cfg, trainer.model, evaluator))\n",
    "        \n",
    "        \n",
    "    def inference(self):\n",
    "        self.cfg.MODEL.WEIGHTS = os.path.join(self.cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "        self.cfg.MODEL.RETINANET.SCORE_THRESH_TEST =  0.5  # set a custom testing threshold\n",
    "        predictor = DefaultPredictor(self.cfg)\n",
    "        self.I2C = {value:key for key, value in self.C2I.items()}\n",
    "        theta=0.9\n",
    "        def calculate_recall(theta):\n",
    "            print(f\"Calculating recall @ {theta}\")\n",
    "            class_recall_map = {class_name:{'num_bboxes':0, 'pred_bboxes':0} for class_name in self.classes}\n",
    "            num_bboxes = 0\n",
    "            pred_bboxes = 0\n",
    "            for d in tqdm.tqdm(DatasetCatalog.get(\"valid\")):\n",
    "                im = cv2.imread(str(d[\"file_name\"]))\n",
    "                for antn_index in range(len(d['annotations'])):\n",
    "                    class_recall_map[self.I2C[d['annotations'][antn_index]['category_id']]]['num_bboxes']+=1\n",
    "                \n",
    "                outputs = predictor(im)\n",
    "\n",
    "                for index,score in enumerate(outputs['instances'].get_fields()['scores']):\n",
    "                    if score.cpu().numpy().item() >= theta :\n",
    "                        for anntn in d['annotations']:\n",
    "                            if anntn['category_id']==outputs['instances'].get_fields()['pred_classes'][index].cpu().numpy().item():\n",
    "                                class_recall_map[self.I2C[anntn['category_id']]]['pred_bboxes']+=1\n",
    "\n",
    "            print(f\"Recall: @{theta}: \\n\")\n",
    "            for cls_name, value in class_recall_map.items():\n",
    "                print(f\" {cls_name}: {value['pred_bboxes']/value['num_bboxes']}, {value['pred_bboxes'], value['num_bboxes']}\")\n",
    "        try:\n",
    "            calculate_recall(0.5)\n",
    "            calculate_recall(0.8)\n",
    "            calculate_recall(0.9)\n",
    "            calculate_recall(0.99)\n",
    "        except:\n",
    "            pass\n",
    "        # for d in random.sample(dataset_dicts, 3):    \n",
    "        #     im = cv2.imread(d[\"file_name\"])\n",
    "        #     outputs = predictor(im)  \n",
    "        #     v = Visualizer(im[:, :, ::-1],\n",
    "        #                 metadata=MetadataCatalog.get(\"train\").set(thing_classes = self.classes), \n",
    "        #                 scale=1.5, \n",
    "        #                 instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "        #     )\n",
    "        #     out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        #     cv2_imshow(out.get_image()[:, :, ::-1])\n",
    "        return predictor\n",
    "            \n",
    "    def test_for_open_set( self, class_df, classes, img_h, img_w, predictor):                                              \n",
    "        data_handler = Data(classes)                                                                                       \n",
    "        protein_domain_data = data_handler.create_protein_domain_data()                                                    \n",
    "        num_rows = protein_domain_data.shape[0]                                                                            \n",
    "        protein_domain_data = protein_domain_data[~protein_domain_data['Sequence'].isin(class_df['Sequence'])]             \n",
    "        print(f\"Dropping {abs(num_rows - protein_domain_data.shape[0])} sequences which are common with {self.classes}'s sequences\")\n",
    "        protein_domain_data = protein_domain_data[protein_domain_data['SeqLen']<img_w]                        \n",
    "        data_handler.create_protein_seq_images(protein_domain_data, img_h, img_w) \n",
    "#         data_handler.create_protein_seq_len_images(protein_domain_data, img_h, img_w)                           \n",
    "        all_imgs = protein_domain_data['img_pth'].tolist()                                                    \n",
    "        print(f\"Using {len(all_imgs)} images from classes {classes} for open set recognition test...........\")\n",
    "        count_0_99 = 0                                                                             \n",
    "        count_0_9 = 0                                                                              \n",
    "        count_0_8 = 0                                                                                                                                \n",
    "        count_0_7 = 0                                                                                                                                \n",
    "        for img_pth in tqdm.tqdm(all_imgs):                                                                                                          \n",
    "            im = cv2.imread(str(img_pth))                                                                                                            \n",
    "            outputs = predictor(im)                                                                                                                  \n",
    "            # v = Visualizer(im[:, :, ::-1],                                                                                                         \n",
    "            #             metadata=MetadataCatalog.get(\"valid\").set(thing_classes = self.classes),                                                   \n",
    "            #             scale=0.5,                                                                                                                 \n",
    "            #             instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models                                                                                                                                     \n",
    "            # )                                                                                                                              \n",
    "            # out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))                                                              \n",
    "            # cv2_imshow(out.get_image()[:, :, ::-1])                                                                                        \n",
    "            try:                                                                                                                             \n",
    "                instance = outputs['instances']                                                                                              \n",
    "                if max(instance.get_fields()['scores']).cpu().numpy() >= 0.99:                                                               \n",
    "                    count_0_99+=1                                                                                                            \n",
    "                elif max(instance.get_fields()['scores']).cpu().numpy() >= 0.9 and max(instance.get_fields()['scores']).cpu().numpy() < 0.99:\n",
    "                    count_0_9+=1                                                                                                            \n",
    "                elif max(instance.get_fields()['scores']).cpu().numpy() >= 0.8 and max(instance.get_fields()['scores']).cpu().numpy() < 0.9:\n",
    "                    count_0_8+=1                                                                                                            \n",
    "                elif max(instance.get_fields()['scores']).cpu().numpy() >= 0.7 and max(instance.get_fields()['scores'].cpu().numpy()) < 0.8:\n",
    "                    count_0_7+=1                                                                                        \n",
    "            except Exception as e:                                                                                      \n",
    "                #print(max(instance.get_fields()['scores']).cpu().numpy())                                              \n",
    "                #print(e)                                                                                               \n",
    "                pass                                                                                                    \n",
    "                                                                                                                        \n",
    "        print(f\"Number of predictions with score >0.99 {count_0_99} out of {len(all_imgs)}: {count_0_99/len(all_imgs)}\")\n",
    "        print(f\"Number of predictions with score >0.9 {count_0_9} out of {len(all_imgs)}: {count_0_9/len(all_imgs)}\")\n",
    "        print(f\"Number of predictions with score >0.8 {count_0_8} out of {len(all_imgs)}: {count_0_8/len(all_imgs)}\")\n",
    "        print(f\"Number of predictions with score >0.7 {count_0_7} out of {len(all_imgs)}: {count_0_7/len(all_imgs)}\")\n",
    "      \n",
    "      \n",
    "\n",
    "        \n",
    "        \n",
    "# if __name__ == \"__main__\":\n",
    "all_classes = ['Amidase_2-PF01510',\n",
    "'Amidase_3-PF01520',]\n",
    "classes=['SH3_3-PF08239','peptidase-PF01433','Lysozyme-PF01183','Lysozyme-PF05838','Lysozyme-PF01374','Lysozyme-PF11860',\n",
    "    'Lysozyme-PF00182','CHAP-PF05257','Lysozyme-PF04965','peptidase-PF05193','Lysozyme-PF00959','SH3_4-PF06347',\n",
    "    'Lysozyme-PF13702','Lysozyme-PF03245','Lysozyme-PF18013']\n",
    " \n",
    "    \n",
    "img_h, img_w = 64, 300 # padding is also controlled by this img_w\n",
    "seq_len =300   \n",
    "seq_buckets = (0, img_w) \n",
    "\n",
    "data_block = Data(classes)\n",
    "protein_data = data_block.create_protein_domain_data()\n",
    "protein_data.to_csv(ProjectRoot/'data/PfamData/less_than_10k_samples_all_seq_len_data.csv', index=False)\n",
    "protein_data = pd.read_csv(ProjectRoot/'data/PfamData/less_than_10k_samples_all_seq_len_data.csv')\n",
    "for col in ['dom', 'dom_len', 'dom_pos']:\n",
    "    protein_data[col] = protein_data[col].apply(lambda x: literal_eval(x))\n",
    "protein_data = protein_data[(protein_data['SeqLen']>=seq_buckets[0]) & (protein_data['SeqLen']<seq_buckets[1])]\n",
    "\n",
    "# filter dom lens which are outliers in each class\n",
    "filtered_indexes = []\n",
    "cls_dom_len_fltr_mp = {'PF08239':(50,60),\n",
    "                       'PF01433':(150,240),\n",
    "                       'PF01183':(160,200),\n",
    "                       'PF05838':(80,100),\n",
    "                       'PF01374':(190,220),\n",
    "                       'PF11860':(160,190),\n",
    "                       'PF00182':(50,240),\n",
    "                       'PF05257':(70,102),\n",
    "                       'PF04965':(85,105),\n",
    "                       'PF05193':(160,200),\n",
    "                       'PF00959':(110,130),\n",
    "                       'PF06347':(52,58),\n",
    "                       'PF13702':(145,175),\n",
    "                       'PF03245':(120,140),\n",
    "                       'PF18013':(120,165)\n",
    "                      }\n",
    "for cls_nm in classes:\n",
    "    cls_id = cls_nm.split('-')[-1]\n",
    "    filtered_indexes.extend(data_block.filter_data(protein_data, cls_id, cls_dom_len_fltr_mp[cls_id][0], cls_dom_len_fltr_mp[cls_id][1]))\n",
    "\n",
    "num_rows = protein_data.shape[0]\n",
    "protein_data = protein_data[protein_data.index.isin(filtered_indexes)]\n",
    "print(f\"Dropped {num_rows-protein_data.shape[0]} sequences based on domain len filtering\") \n",
    "    \n",
    "# # Select majority domains from each class based on dom len dist\n",
    "# # CHAP>>60-120 SH3_4>>50-63 SH3_3>>45-75\n",
    "# # chap_indexs = []\n",
    "# # chap_data = protein_data[protein_data['Class'].str.contains('PF05257')]\n",
    "# # for index, row in chap_data.iterrows():\n",
    "# #     flag=False\n",
    "# #     for dom_len in row['dom_len']:\n",
    "# #         if dom_len >= 70 and dom_len <=102:\n",
    "# #             flag=True\n",
    "# #         else:\n",
    "# #             flag=False\n",
    "# #     if flag:\n",
    "# #         chap_indexs.append(index)\n",
    "        \n",
    "# # sh3_4_indexs = []\n",
    "# # sh3_4_data = protein_data[protein_data['Class'].str.contains('PF06347')]\n",
    "# # for index, row in sh3_4_data.iterrows():\n",
    "# #     flag=False\n",
    "# #     for dom_len in row['dom_len']:\n",
    "# #         if dom_len >=52 and dom_len <=58:\n",
    "# #             flag=True\n",
    "# #         else:\n",
    "# #             flag=False\n",
    "# #     if flag:\n",
    "# #         sh3_4_indexs.append(index)\n",
    "            \n",
    "# # sh3_3_indexs = []\n",
    "# # sh3_3_data = protein_data[protein_data['Class'].str.contains('PF08239')]\n",
    "# # for index, row in sh3_3_data.iterrows():\n",
    "# #     flag=False\n",
    "# #     for dom_len in row['dom_len']:\n",
    "# #         if dom_len >=50 and dom_len <=60:\n",
    "# #             flag=True\n",
    "# #         else:\n",
    "# #             flag=False\n",
    "# #     if flag:\n",
    "# #         sh3_3_indexs.append(index)\n",
    "\n",
    "# # num_rows = protein_data.shape[0]\n",
    "# # protein_data = protein_data[protein_data.index.isin(chap_indexs)]#+sh3_3_indexs+sh3_4_indexs)]\n",
    "# # print(f\"Number of records dropeed after selecting major domains based on len: {num_rows - protein_data.shape[0]}\")\n",
    "            \n",
    "# data_block.create_protein_seq_images(protein_data, img_h, img_w)\n",
    "# # data_block.create_protein_seq_len_images(protein_data, img_h, img_w)\n",
    "\n",
    "# model_block = Detectron(classes=classes, model_dir=ProjectRoot/f\"models/{'_'.join(classes)}_{seq_buckets[0]}_{seq_buckets[1]}\",img_h=img_h,img_w=img_w)\n",
    "# model_block.cfg.SOLVER.MAX_ITER = 3000\n",
    "# train_data, valid_data = model_block.create_train_valid_data(protein_data)\n",
    "# model_block.register_custom_data(train_data, 'train', img_h, img_w)\n",
    "# model_block.register_custom_data(valid_data, 'valid', img_h, img_w)\n",
    "# trainer = model_block.train()\n",
    "# model_block.evaluate(trainer)\n",
    "# predictor = model_block.inference()\n",
    "# #class_index = all_classes.index(classes[0])\n",
    "# #del all_classes[class_index]\n",
    "# model_block.test_for_open_set(protein_data, all_classes, img_h, img_w, predictor)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check class data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi GPU training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.engine import DefaultTrainer, default_argument_parser, default_setup, hooks, launch\n",
    "from detectron2.evaluation import (\n",
    "    CityscapesInstanceEvaluator,\n",
    "    CityscapesSemSegEvaluator,\n",
    "    COCOEvaluator,\n",
    "    COCOPanopticEvaluator,\n",
    "    DatasetEvaluators,\n",
    "    LVISEvaluator,\n",
    "    PascalVOCDetectionEvaluator,\n",
    "    SemSegEvaluator,\n",
    "    verify_results,\n",
    ")\n",
    "from detectron2.modeling import GeneralizedRCNNWithTTA\n",
    "\n",
    "\n",
    "class Trainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    We use the \"DefaultTrainer\" which contains pre-defined default logic for\n",
    "    standard training workflow. They may not work for you, especially if you\n",
    "    are working on a new research project. In that case you can write your\n",
    "    own training loop. You can use \"tools/plain_train_net.py\" as an example.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        \"\"\"\n",
    "        Create evaluator(s) for a given dataset.\n",
    "        This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n",
    "        For your own dataset, you can simply create an evaluator manually in your\n",
    "        script and do not have to worry about the hacky if-else logic here.\n",
    "        \"\"\"\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        \n",
    "        return COCOEvaluator(dataset_name, output_dir=output_folder)\n",
    "\n",
    "    @classmethod\n",
    "    def test_with_TTA(cls, cfg, model):\n",
    "        logger = logging.getLogger(\"detectron2.trainer\")\n",
    "        # In the end of training, run an evaluation with TTA\n",
    "        # Only support some R-CNN models.\n",
    "        logger.info(\"Running inference with test-time augmentation ...\")\n",
    "        model = GeneralizedRCNNWithTTA(cfg, model)\n",
    "        evaluators = [\n",
    "            cls.build_evaluator(\n",
    "                cfg, name, output_folder=os.path.join(cfg.OUTPUT_DIR, \"inference_TTA\")\n",
    "            )\n",
    "            for name in cfg.DATASETS.TEST\n",
    "        ]\n",
    "        res = cls.test(cfg, model, evaluators)\n",
    "        res = OrderedDict({k + \"_TTA\": v for k, v in res.items()})\n",
    "        return res\n",
    "\n",
    "\n",
    "def setup(args):\n",
    "    \"\"\"\n",
    "    Create configs and perform basic setups.\n",
    "    \"\"\"\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    cfg.freeze()\n",
    "    default_setup(cfg, args)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    cfg = setup(args)\n",
    "\n",
    "    if args.eval_only:\n",
    "        model = Trainer.build_model(cfg)\n",
    "        DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n",
    "            cfg.MODEL.WEIGHTS, resume=args.resume\n",
    "        )\n",
    "        res = Trainer.test(cfg, model)\n",
    "        if cfg.TEST.AUG.ENABLED:\n",
    "            res.update(Trainer.test_with_TTA(cfg, model))\n",
    "        if comm.is_main_process():\n",
    "            verify_results(cfg, res)\n",
    "        return res\n",
    "\n",
    "    \"\"\"\n",
    "    If you'd like to do anything fancier than the standard training logic,\n",
    "    consider writing your own training loop (see plain_train_net.py) or\n",
    "    subclassing the trainer.\n",
    "    \"\"\"\n",
    "    trainer = Trainer(cfg)\n",
    "    trainer.resume_or_load(resume=args.resume)\n",
    "    if cfg.TEST.AUG.ENABLED:\n",
    "        trainer.register_hooks(\n",
    "            [hooks.EvalHook(0, lambda: trainer.test_with_TTA(cfg, trainer.model))]\n",
    "        )\n",
    "    trainer.train()\n",
    "    evaluator = COCOEvaluator(\"valid\", (\"bbox\",), False, output_dir=cfg.OUTPUT_DIR )\n",
    "    val_loader = build_detection_test_loader(cfg, \"valid\")\n",
    "    print(trainer.test(cfg, trainer.model, evaluator))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = default_argument_parser().parse_args()\n",
    "    print(\"Command Line Args:\", args)\n",
    "    launch(\n",
    "        main,\n",
    "        args.num_gpus,\n",
    "        num_machines=args.num_machines,\n",
    "        machine_rank=args.machine_rank,\n",
    "        dist_url=args.dist_url,\n",
    "        args=(args,),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "detectron2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

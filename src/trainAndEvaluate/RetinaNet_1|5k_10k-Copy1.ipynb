{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes with samples between 1.5k to 10k\n",
    "\n",
    "# classes = ['Lysozyme-PF03245',\n",
    "#  'Lysozyme-PF16754',\n",
    "#  'Lysozyme-PF11860',\n",
    "#  'Lysozyme-PF13702',\n",
    "#  'Lysozyme-PF00959',\n",
    "#  'Lysozyme-PF00182',\n",
    "#  'Lysozyme-PF00704',\n",
    "#  'Lysozyme-PF01374',\n",
    "#  'Lysozyme-PF05838',\n",
    "#  'Lysozyme-PF18013',\n",
    "#  'Lysozyme-PF04965',\n",
    "#  'Lysozyme-PF01183',\n",
    "#  'Lysozyme-PF00722',\n",
    "#  'peptidase-PF05193',\n",
    "#  'peptidase-PF01551',\n",
    "#  'peptidase-PF00675',\n",
    "#  'peptidase-PF01435',\n",
    "#  'peptidase-PF01433',\n",
    "#  'peptidase-PF10502',\n",
    "#  'peptidase-PF00246',\n",
    "#  'peptidase-PF03572',\n",
    "#  'peptidase-PF00814',\n",
    "#  'peptidase-PF17900',\n",
    "#  'Amidase_2-PF01510',\n",
    "#  'Amidase_3-PF01520',\n",
    "#  'CHAP-PF05257',\n",
    "#  'SH3_4-PF06347',\n",
    "#  'SH3_3-PF08239',\n",
    "#  'SH3_5-PF08460',\n",
    "#  'LysM-PF01476']\n",
    "\n",
    "classes= ['CHAP-PF05257','SH3_4-PF06347',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "import sys\n",
    "from pathlib import Path\n",
    "ProjectRoot = Path().cwd().parent.parent\n",
    "sys.path.append(str(ProjectRoot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.modelData import ObjectDetection\n",
    "# #! trash-put /home/satish27may/ProteinDomainDetection/data/PfamData/images_224/\n",
    "# #! trash-put /home/satish27may/ProteinDomainDetection/data/PfamData/annotations_224/\n",
    "# data_handler = ObjectDetection(class_names=classes, img_dim=224)\n",
    "# #data_handler.create_coco_data(augment_data=False, num_augs=1000, img_h=224, img_w=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/IPython/utils/traitlets.py:5: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  warn(\"IPython.utils.traitlets has moved to a top-level traitlets package.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3222/3222 [00:00<00:00, 22318.09it/s]\n",
      "  0%|          | 0/1380 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes selected: ['PF05257' 'PF06347']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1380/1380 [00:00<00:00, 21521.57it/s]\n",
      " 38%|███▊      | 1233/3222 [00:00<00:00, 12327.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes selected: ['PF05257' 'PF06347']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3222/3222 [00:00<00:00, 17025.74it/s]\n",
      "100%|██████████| 1380/1380 [00:00<00:00, 22323.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def create_train_valid_test_data(data_df):\n",
    "    train_dfs,valid_dfs = [],[],\n",
    "    for class_name in data_df['Class'].unique():\n",
    "        class_df = data_df[data_df['Class']==class_name].sample(frac=1)\n",
    "        num_samples = class_df.shape[0]\n",
    "        num_train_samples = int(round(num_samples*0.7))\n",
    "        train_dfs.append(class_df.iloc[:num_train_samples,:])\n",
    "        valid_dfs.append(class_df.iloc[num_train_samples:,:])\n",
    "    return pd.concat(train_dfs,axis='rows').sample(frac=1), pd.concat(valid_dfs,axis='rows').sample(frac=1)\n",
    "    \n",
    "def create_dataset(img_h, img_w, mode, classes):\n",
    "    model_data = pd.read_csv(ProjectRoot/'data/PfamData/model_data.csv')\n",
    "    model_data = model_data[model_data['Class'].isin([x.split('-')[-1] for x in classes])]\n",
    "    print(f\"Classes selected: {model_data['Class'].unique()}\")\n",
    "    model_data = model_data.reset_index(drop=True)\n",
    "    model_data['dom_pos'] = model_data['dom_pos'].apply(lambda x: [int(y) for y in x.replace('[','').replace(']','').split(',')])\n",
    "    C2I = {class_name:index for index, class_name in enumerate(model_data['Class'].unique())}\n",
    "    train_dicts_list = []\n",
    "    valid_dicts_list = []\n",
    "    train, valid = create_train_valid_test_data(model_data)\n",
    "    train = train.reset_index(drop=True)\n",
    "    valid = valid.reset_index(drop=True)\n",
    "    for index in tqdm(range(train.shape[0])):\n",
    "        x1,x2 = train['dom_pos'][index]\n",
    "        train_dicts_list.append({'file_name':train['img_pth'][index],\n",
    "                           'height':img_h,\n",
    "                           'width': img_w,\n",
    "                           'image_id': index,\n",
    "                           'annotations':[{'bbox':[x1, 0, x2, img_h],\n",
    "                                           'bbox_mode':BoxMode.XYXY_ABS,\n",
    "                                           'category_id':  C2I[train['Class'][index]],\n",
    "                                          }]\n",
    "                          })\n",
    "    for index in tqdm(range(valid.shape[0])):\n",
    "        x1,x2 = valid['dom_pos'][index]\n",
    "        valid_dicts_list.append({'file_name':valid['img_pth'][index],\n",
    "                           'height':img_h,\n",
    "                           'width': img_w,\n",
    "                           'image_id': index,\n",
    "                           'annotations':[{'bbox':[x1, 0, x2, img_h],\n",
    "                                           'bbox_mode':BoxMode.XYXY_ABS,\n",
    "                                           'category_id':  C2I[valid['Class'][index]],\n",
    "                                          }]\n",
    "                          })\n",
    "    if mode=='train':return train_dicts_list\n",
    "    elif mode=='valid': return valid_dicts_list\n",
    "\n",
    "train_list = create_dataset(img_h=224, img_w=1000, mode='train', classes=classes)\n",
    "valid_list = create_dataset(img_h=224, img_w=1000, mode='valid', classes=classes)\n",
    "def get_train_data():\n",
    "    return train_list\n",
    "\n",
    "def get_valid_data():\n",
    "    return valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetCatalog.register(\"train\", get_train_data)\n",
    "DatasetCatalog.register(\"valid\", get_valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from detectron2.data.datasets import register_coco_instances\n",
    "# # images used should be at 224x1000 dim\n",
    "# register_coco_instances(\"ballon_train\", {}, \"/home/satish27may/ProteinDomainDetection/data/PfamData/annotations_224/train.json\", \"/home/satish27may/ProteinDomainDetection/data/PfamData/images_224/\")\n",
    "# register_coco_instances(\"ballon_val\", {}, \"/home/satish27may/ProteinDomainDetection/data/PfamData/annotations_224/val.json\", \"/home/satish27may/ProteinDomainDetection/data/PfamData/images_224/\")\n",
    "# # register_coco_instances(\"ballon_test\", {}, \"/home/satish27may/ProteinDomainDetection/data/PfamData/annotations_224//test.json\", \"/home/satish27may/ProteinDomainDetection/data/PfamData/images_224/\")\n",
    "# balloon_metadata = MetadataCatalog.get(\"balloon_train\")\n",
    "\n",
    "# ! mkdir /home/satish27may/ProteinDomainDetection/data/PfamData/torch_dataset\n",
    "# ! trash-put /home/satish27may/ProteinDomainDetection/data/PfamData/torch_dataset/class1\n",
    "# ! trash-put /home/satish27may/ProteinDomainDetection/data/PfamData/torch_dataset/images_224\n",
    "# ! mkdir /home/satish27may/ProteinDomainDetection/data/PfamData/torch_dataset/class1\n",
    "# ! cp  -r /home/satish27may/ProteinDomainDetection/data/PfamData/images_224/  /home/satish27may/ProteinDomainDetection/data/PfamData/torch_dataset/class1/\n",
    "# #! mv /home/satish27may/ProteinDomainDetection/data/PfamData/torch_dataset/images_224 /home/satish27may/DetectoRS_mmdetect/mmdetection/data/torch_dataset/class1\n",
    "# def get_mean_std(loader):\n",
    "#     # var[X] = E[X**2] - E[X]**2\n",
    "#     channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n",
    "\n",
    "#     for data, _ in tqdm(loader):\n",
    "#         channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "#         channels_sqrd_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
    "#         num_batches += 1\n",
    "\n",
    "#     mean = channels_sum / num_batches\n",
    "#     std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5\n",
    "\n",
    "#     return mean, std\n",
    "\n",
    "\n",
    "# custom_dataset = datasets.ImageFolder('/home/satish27may/ProteinDomainDetection/data/PfamData/torch_dataset', transform=transforms.ToTensor())\n",
    "# custom_data_loader = DataLoader(dataset=custom_dataset,batch_size=128)\n",
    "# data_mean, data_std = get_mean_std(custom_data_loader)\n",
    "\n",
    "# data_mean = data_mean.numpy().tolist()\n",
    "# data_std = data_std.numpy().tolist()\n",
    "\n",
    "# dataset_dicts = DatasetCatalog.get(\"ballon_train\")\n",
    "# for d in random.sample(dataset_dicts, 3):\n",
    "#     print(d)\n",
    "#     img = cv2.imread(d[\"file_name\"])\n",
    "#     visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=1.5)\n",
    "#     bbox = d['annotations'][0]['bbox']\n",
    "#     out = visualizer.draw_box((bbox[0],bbox[1],bbox[0]+bbox[2],bbox[1]+bbox[3]), edge_color='red', line_style='-')\n",
    "#     cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import MetadataCatalog\n",
    "MetadataCatalog.get(\"train\").thing_classes = classes\n",
    "MetadataCatalog.get(\"valid\").thing_classes = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! trash-put /home/satish27may/ProteinDomainDetection/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading config /home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/detectron2/model_zoo/configs/COCO-Detection/../Base-RetinaNet.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/26 04:58:54 d2.engine.defaults]: \u001b[0mModel:\n",
      "RetinaNet(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelP6P7(\n",
      "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): RetinaNetHead(\n",
      "    (cls_subnet): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (bbox_subnet): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU()\n",
      "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU()\n",
      "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (cls_score): Conv2d(256, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (anchor_generator): DefaultAnchorGenerator(\n",
      "    (cell_anchors): BufferList()\n",
      "  )\n",
      ")\n",
      "\u001b[32m[11/26 04:58:54 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 3222 images left.\n",
      "\u001b[32m[11/26 04:58:54 d2.data.build]: \u001b[0mDistribution of instances among all 2 categories:\n",
      "\u001b[36m|   category   | #instances   |   category    | #instances   |\n",
      "|:------------:|:-------------|:-------------:|:-------------|\n",
      "| CHAP-PF05257 | 2150         | SH3_4-PF06347 | 1072         |\n",
      "|              |              |               |              |\n",
      "|    total     | 3222         |               |              |\u001b[0m\n",
      "\u001b[32m[11/26 04:58:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip(horizontal=False, vertical=True)]\n",
      "\u001b[32m[11/26 04:58:54 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[11/26 04:58:54 d2.data.common]: \u001b[0mSerializing 3222 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/26 04:58:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.88 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'head.cls_score.weight' to the model due to incompatible shapes: (720, 256, 3, 3) in the checkpoint but (18, 256, 3, 3) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'head.cls_score.bias' to the model due to incompatible shapes: (720,) in the checkpoint but (18,) in the model! You might want to double check if this is expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/26 04:58:56 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n",
      "/home/satish27may/anaconda3/envs/detectron2/lib/python3.7/site-packages/fvcore/transforms/transform.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  tensor = torch.from_numpy(np.ascontiguousarray(img))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/26 04:59:11 d2.utils.events]: \u001b[0m eta: 0:03:15  iter: 19  total_loss: 0.7764  loss_cls: 0.5911  loss_box_reg: 0.2191  time: 0.7277  data_time: 0.0290  lr: 1.9784e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 04:59:25 d2.utils.events]: \u001b[0m eta: 0:03:01  iter: 39  total_loss: 1.204  loss_cls: 0.8831  loss_box_reg: 0.3427  time: 0.7115  data_time: 0.0049  lr: 3.8318e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 04:59:39 d2.utils.events]: \u001b[0m eta: 0:02:47  iter: 59  total_loss: 0.8387  loss_cls: 0.5997  loss_box_reg: 0.2622  time: 0.7062  data_time: 0.0045  lr: 5.44e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 04:59:53 d2.utils.events]: \u001b[0m eta: 0:02:33  iter: 79  total_loss: 0.6939  loss_cls: 0.4233  loss_box_reg: 0.2131  time: 0.7033  data_time: 0.0045  lr: 6.7009e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 05:00:07 d2.utils.events]: \u001b[0m eta: 0:02:19  iter: 99  total_loss: 0.6463  loss_cls: 0.402  loss_box_reg: 0.2439  time: 0.7010  data_time: 0.0047  lr: 7.5377e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 05:00:21 d2.utils.events]: \u001b[0m eta: 0:02:05  iter: 119  total_loss: 0.5325  loss_cls: 0.3499  loss_box_reg: 0.2012  time: 0.6996  data_time: 0.0048  lr: 7.9059e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 05:00:35 d2.utils.events]: \u001b[0m eta: 0:01:51  iter: 139  total_loss: 0.5221  loss_cls: 0.3236  loss_box_reg: 0.1783  time: 0.6987  data_time: 0.0053  lr: 7.7968e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 05:00:49 d2.utils.events]: \u001b[0m eta: 0:01:37  iter: 159  total_loss: 0.5369  loss_cls: 0.3188  loss_box_reg: 0.197  time: 0.6980  data_time: 0.0047  lr: 7.2399e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 05:01:03 d2.utils.events]: \u001b[0m eta: 0:01:23  iter: 179  total_loss: 0.5039  loss_cls: 0.3067  loss_box_reg: 0.1892  time: 0.6974  data_time: 0.0046  lr: 6.3024e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 05:01:16 d2.utils.events]: \u001b[0m eta: 0:01:09  iter: 199  total_loss: 0.4606  loss_cls: 0.2828  loss_box_reg: 0.1828  time: 0.6969  data_time: 0.0046  lr: 5.0859e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 05:01:30 d2.utils.events]: \u001b[0m eta: 0:00:55  iter: 219  total_loss: 0.5244  loss_cls: 0.3343  loss_box_reg: 0.1994  time: 0.6965  data_time: 0.0047  lr: 3.7219e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 05:01:44 d2.utils.events]: \u001b[0m eta: 0:00:41  iter: 239  total_loss: 0.4589  loss_cls: 0.3191  loss_box_reg: 0.1724  time: 0.6962  data_time: 0.0047  lr: 2.3638e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 05:01:58 d2.utils.events]: \u001b[0m eta: 0:00:27  iter: 259  total_loss: 0.4688  loss_cls: 0.2895  loss_box_reg: 0.1731  time: 0.6959  data_time: 0.0046  lr: 1.1788e-05  max_mem: 1164M\n",
      "\u001b[32m[11/26 05:02:12 d2.utils.events]: \u001b[0m eta: 0:00:13  iter: 279  total_loss: 0.4977  loss_cls: 0.3113  loss_box_reg: 0.1626  time: 0.6958  data_time: 0.0048  lr: 3.3683e-06  max_mem: 1164M\n",
      "\u001b[32m[11/26 05:02:27 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 299  total_loss: 0.4141  loss_cls: 0.2586  loss_box_reg: 0.1507  time: 0.6956  data_time: 0.0047  lr: 8.2164e-09  max_mem: 1164M\n",
      "\u001b[32m[11/26 05:02:27 d2.engine.hooks]: \u001b[0mOverall training speed: 298 iterations in 0:03:27 (0.6956 s / it)\n",
      "\u001b[32m[11/26 05:02:27 d2.engine.hooks]: \u001b[0mTotal training time: 0:03:28 (0:00:01 on hooks)\n",
      "\u001b[32m[11/26 05:02:27 d2.data.build]: \u001b[0mDistribution of instances among all 2 categories:\n",
      "\u001b[36m|   category   | #instances   |   category    | #instances   |\n",
      "|:------------:|:-------------|:-------------:|:-------------|\n",
      "| CHAP-PF05257 | 921          | SH3_4-PF06347 | 459          |\n",
      "|              |              |               |              |\n",
      "|    total     | 1380         |               |              |\u001b[0m\n",
      "\u001b[32m[11/26 05:02:27 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[11/26 05:02:27 d2.data.common]: \u001b[0mSerializing 1380 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/26 05:02:27 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[11/26 05:02:27 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import CfgNode as CN\n",
    "img_h, img_w = 224, 1000\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"train\",)\n",
    "cfg.DATASETS.TEST = (\"valid\",)\n",
    "#cfg.MODEL.PIXEL_MEAN = data_mean\n",
    "#cfg.MODEL.PIXEL_STD = data_std\n",
    "cfg.INPUT.RANDOM_FLIP = \"vertical\"\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 100\n",
    "\n",
    "\n",
    "cfg.TEST.AUG.FLIP = False\n",
    "cfg.DATALOADER.NUM_WORKERS = 8\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 1e-3  # pick a good LR\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\n",
    "#cfg.MODEL.RETINANET.IOU_THRESHOLDS = [0.4, 0.5]\n",
    "cfg.SOLVER.MAX_ITER = 300\n",
    "cfg.MODEL.RETINANET.NUM_CLASSES = len(classes)\n",
    "# cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(data_handler.class_names)\n",
    "\n",
    "cfg.OUTPUT_DIR = '/home/satish27may/ProteinDomainDetection/models'\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[11/26 05:02:27 d2.evaluation.coco_evaluation]: \u001b[0m'valid' is not registered by `register_coco_instances`. Therefore trying to convert it to COCO format ...\n",
      "\u001b[32m[11/26 05:02:27 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'valid' to COCO format ...)\n",
      "\u001b[32m[11/26 05:02:27 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n",
      "\u001b[32m[11/26 05:02:27 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 1380, #annotations: 1380\n",
      "\u001b[32m[11/26 05:02:27 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at '/home/satish27may/ProteinDomainDetection/models/valid_coco_format.json' ...\n",
      "\u001b[32m[11/26 05:02:27 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[11/26 05:02:27 d2.data.common]: \u001b[0mSerializing 1380 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/26 05:02:27 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
      "\u001b[32m[11/26 05:02:27 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[11/26 05:02:27 d2.data.common]: \u001b[0mSerializing 1380 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[11/26 05:02:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.38 MiB\n",
      "\u001b[32m[11/26 05:02:28 d2.evaluation.evaluator]: \u001b[0mStart inference on 1380 images\n",
      "\u001b[32m[11/26 05:02:30 d2.evaluation.evaluator]: \u001b[0mInference done 11/1380. 0.1797 s / img. ETA=0:04:08\n",
      "\u001b[32m[11/26 05:02:35 d2.evaluation.evaluator]: \u001b[0mInference done 39/1380. 0.1806 s / img. ETA=0:04:04\n",
      "\u001b[32m[11/26 05:02:40 d2.evaluation.evaluator]: \u001b[0mInference done 67/1380. 0.1804 s / img. ETA=0:03:59\n",
      "\u001b[32m[11/26 05:02:45 d2.evaluation.evaluator]: \u001b[0mInference done 95/1380. 0.1806 s / img. ETA=0:03:54\n",
      "\u001b[32m[11/26 05:02:50 d2.evaluation.evaluator]: \u001b[0mInference done 123/1380. 0.1807 s / img. ETA=0:03:49\n",
      "\u001b[32m[11/26 05:02:55 d2.evaluation.evaluator]: \u001b[0mInference done 151/1380. 0.1808 s / img. ETA=0:03:44\n",
      "\u001b[32m[11/26 05:03:01 d2.evaluation.evaluator]: \u001b[0mInference done 179/1380. 0.1808 s / img. ETA=0:03:39\n",
      "\u001b[32m[11/26 05:03:06 d2.evaluation.evaluator]: \u001b[0mInference done 207/1380. 0.1809 s / img. ETA=0:03:34\n",
      "\u001b[32m[11/26 05:03:11 d2.evaluation.evaluator]: \u001b[0mInference done 235/1380. 0.1809 s / img. ETA=0:03:29\n",
      "\u001b[32m[11/26 05:03:16 d2.evaluation.evaluator]: \u001b[0mInference done 263/1380. 0.1809 s / img. ETA=0:03:23\n",
      "\u001b[32m[11/26 05:03:21 d2.evaluation.evaluator]: \u001b[0mInference done 291/1380. 0.1809 s / img. ETA=0:03:18\n",
      "\u001b[32m[11/26 05:03:26 d2.evaluation.evaluator]: \u001b[0mInference done 319/1380. 0.1809 s / img. ETA=0:03:13\n",
      "\u001b[32m[11/26 05:03:31 d2.evaluation.evaluator]: \u001b[0mInference done 347/1380. 0.1810 s / img. ETA=0:03:08\n",
      "\u001b[32m[11/26 05:03:36 d2.evaluation.evaluator]: \u001b[0mInference done 375/1380. 0.1810 s / img. ETA=0:03:03\n",
      "\u001b[32m[11/26 05:03:42 d2.evaluation.evaluator]: \u001b[0mInference done 403/1380. 0.1810 s / img. ETA=0:02:58\n",
      "\u001b[32m[11/26 05:03:47 d2.evaluation.evaluator]: \u001b[0mInference done 431/1380. 0.1809 s / img. ETA=0:02:53\n",
      "\u001b[32m[11/26 05:03:52 d2.evaluation.evaluator]: \u001b[0mInference done 459/1380. 0.1809 s / img. ETA=0:02:48\n",
      "\u001b[32m[11/26 05:03:57 d2.evaluation.evaluator]: \u001b[0mInference done 487/1380. 0.1809 s / img. ETA=0:02:43\n",
      "\u001b[32m[11/26 05:04:02 d2.evaluation.evaluator]: \u001b[0mInference done 515/1380. 0.1809 s / img. ETA=0:02:37\n",
      "\u001b[32m[11/26 05:04:07 d2.evaluation.evaluator]: \u001b[0mInference done 543/1380. 0.1809 s / img. ETA=0:02:32\n",
      "\u001b[32m[11/26 05:04:12 d2.evaluation.evaluator]: \u001b[0mInference done 571/1380. 0.1809 s / img. ETA=0:02:27\n",
      "\u001b[32m[11/26 05:04:17 d2.evaluation.evaluator]: \u001b[0mInference done 599/1380. 0.1809 s / img. ETA=0:02:22\n",
      "\u001b[32m[11/26 05:04:22 d2.evaluation.evaluator]: \u001b[0mInference done 627/1380. 0.1810 s / img. ETA=0:02:17\n",
      "\u001b[32m[11/26 05:04:28 d2.evaluation.evaluator]: \u001b[0mInference done 654/1380. 0.1810 s / img. ETA=0:02:12\n",
      "\u001b[32m[11/26 05:04:33 d2.evaluation.evaluator]: \u001b[0mInference done 682/1380. 0.1810 s / img. ETA=0:02:07\n",
      "\u001b[32m[11/26 05:04:38 d2.evaluation.evaluator]: \u001b[0mInference done 710/1380. 0.1810 s / img. ETA=0:02:02\n",
      "\u001b[32m[11/26 05:04:43 d2.evaluation.evaluator]: \u001b[0mInference done 738/1380. 0.1810 s / img. ETA=0:01:57\n",
      "\u001b[32m[11/26 05:04:48 d2.evaluation.evaluator]: \u001b[0mInference done 766/1380. 0.1810 s / img. ETA=0:01:52\n",
      "\u001b[32m[11/26 05:04:53 d2.evaluation.evaluator]: \u001b[0mInference done 794/1380. 0.1810 s / img. ETA=0:01:47\n",
      "\u001b[32m[11/26 05:04:58 d2.evaluation.evaluator]: \u001b[0mInference done 822/1380. 0.1810 s / img. ETA=0:01:42\n",
      "\u001b[32m[11/26 05:05:03 d2.evaluation.evaluator]: \u001b[0mInference done 850/1380. 0.1810 s / img. ETA=0:01:36\n",
      "\u001b[32m[11/26 05:05:08 d2.evaluation.evaluator]: \u001b[0mInference done 878/1380. 0.1810 s / img. ETA=0:01:31\n",
      "\u001b[32m[11/26 05:05:14 d2.evaluation.evaluator]: \u001b[0mInference done 906/1380. 0.1810 s / img. ETA=0:01:26\n",
      "\u001b[32m[11/26 05:05:19 d2.evaluation.evaluator]: \u001b[0mInference done 934/1380. 0.1809 s / img. ETA=0:01:21\n",
      "\u001b[32m[11/26 05:05:24 d2.evaluation.evaluator]: \u001b[0mInference done 962/1380. 0.1809 s / img. ETA=0:01:16\n",
      "\u001b[32m[11/26 05:05:29 d2.evaluation.evaluator]: \u001b[0mInference done 990/1380. 0.1809 s / img. ETA=0:01:11\n",
      "\u001b[32m[11/26 05:05:34 d2.evaluation.evaluator]: \u001b[0mInference done 1018/1380. 0.1809 s / img. ETA=0:01:06\n",
      "\u001b[32m[11/26 05:05:39 d2.evaluation.evaluator]: \u001b[0mInference done 1046/1380. 0.1809 s / img. ETA=0:01:01\n",
      "\u001b[32m[11/26 05:05:44 d2.evaluation.evaluator]: \u001b[0mInference done 1074/1380. 0.1809 s / img. ETA=0:00:55\n",
      "\u001b[32m[11/26 05:05:49 d2.evaluation.evaluator]: \u001b[0mInference done 1102/1380. 0.1809 s / img. ETA=0:00:50\n",
      "\u001b[32m[11/26 05:05:54 d2.evaluation.evaluator]: \u001b[0mInference done 1130/1380. 0.1809 s / img. ETA=0:00:45\n",
      "\u001b[32m[11/26 05:06:00 d2.evaluation.evaluator]: \u001b[0mInference done 1158/1380. 0.1809 s / img. ETA=0:00:40\n",
      "\u001b[32m[11/26 05:06:05 d2.evaluation.evaluator]: \u001b[0mInference done 1186/1380. 0.1809 s / img. ETA=0:00:35\n",
      "\u001b[32m[11/26 05:06:10 d2.evaluation.evaluator]: \u001b[0mInference done 1214/1380. 0.1809 s / img. ETA=0:00:30\n",
      "\u001b[32m[11/26 05:06:15 d2.evaluation.evaluator]: \u001b[0mInference done 1242/1380. 0.1809 s / img. ETA=0:00:25\n",
      "\u001b[32m[11/26 05:06:20 d2.evaluation.evaluator]: \u001b[0mInference done 1270/1380. 0.1809 s / img. ETA=0:00:20\n",
      "\u001b[32m[11/26 05:06:25 d2.evaluation.evaluator]: \u001b[0mInference done 1298/1380. 0.1809 s / img. ETA=0:00:14\n",
      "\u001b[32m[11/26 05:06:30 d2.evaluation.evaluator]: \u001b[0mInference done 1326/1380. 0.1809 s / img. ETA=0:00:09\n",
      "\u001b[32m[11/26 05:06:35 d2.evaluation.evaluator]: \u001b[0mInference done 1354/1380. 0.1809 s / img. ETA=0:00:04\n",
      "\u001b[32m[11/26 05:06:40 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:04:11.239008 (0.182719 s / img per device, on 1 devices)\n",
      "\u001b[32m[11/26 05:06:40 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:04:08 (0.180849 s / img per device, on 1 devices)\n",
      "\u001b[32m[11/26 05:06:40 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[11/26 05:06:40 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to /home/satish27may/ProteinDomainDetection/models/coco_instances_results.json\n",
      "\u001b[32m[11/26 05:06:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.18s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "COCOeval_opt.evaluate() finished in 0.37 seconds.\n",
      "Accumulating evaluation results...\n",
      "COCOeval_opt.accumulate() finished in 0.10 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.143\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.256\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.145\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.144\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.262\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.598\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.604\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.100\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.605\n",
      "\u001b[32m[11/26 05:06:41 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 14.337 | 25.600 | 14.487 |  nan  | 2.500 | 14.353 |\n",
      "\u001b[32m[11/26 05:06:41 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[11/26 05:06:41 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category     | AP     | category      | AP    |\n",
      "|:-------------|:-------|:--------------|:------|\n",
      "| CHAP-PF05257 | 25.929 | SH3_4-PF06347 | 2.744 |\n",
      "\u001b[32m[11/26 05:06:41 d2.engine.defaults]: \u001b[0mEvaluation results for valid in csv format:\n",
      "\u001b[32m[11/26 05:06:41 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[11/26 05:06:41 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[11/26 05:06:41 d2.evaluation.testing]: \u001b[0mcopypaste: 14.3369,25.6001,14.4872,nan,2.5000,14.3526\n",
      "OrderedDict([('bbox', {'AP': 14.33690018588213, 'AP50': 25.600112129718084, 'AP75': 14.48720243585514, 'APs': nan, 'APm': 2.5, 'APl': 14.352648338368764, 'AP-CHAP-PF05257': 25.92946580568646, 'AP-SH3_4-PF06347': 2.7443345660778022})])\n"
     ]
    }
   ],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "evaluator = COCOEvaluator(\"valid\", (\"bbox\",), False, output_dir=cfg.OUTPUT_DIR )\n",
    "val_loader = build_detection_test_loader(cfg, \"valid\")\n",
    "print(trainer.test(cfg, trainer.model, evaluator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im)\n",
    "v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "| category     | AP     | category      | AP     |\n",
    "|:-------------|:-------|:--------------|:-------|\n",
    "| CHAP-PF05257 | 55.940 | SH3_4-PF06347 | 21.454 |\n",
    " \n",
    "\n",
    "\n",
    "| category   | AP     | category   | AP     |\n",
    "|:-----------|:-------|:-----------|:-------|\n",
    "| PF05257    | 41.624 | PF06347    | 23.025 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "COCOeval_opt.accumulate() finished in 0.62 seconds.\n",
    "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.343\n",
    "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.450\n",
    "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.383\n",
    "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
    "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
    "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.344\n",
    "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.581\n",
    "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.730\n",
    "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.733\n",
    "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
    "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
    "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.733\n",
    "[11/23 10:26:47 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
    "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
    "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
    "| 34.336 | 45.023 | 38.327 |  nan  | 20.099 | 34.352 |\n",
    "[11/23 10:26:47 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
    "[11/23 10:26:47 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
    "| category   | AP     | category   | AP     | category   | AP     |\n",
    "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
    "| PF01183    | 54.320 | PF05257    | 11.621 | PF00959    | 28.577 |\n",
    "| PF00182    | 35.231 | PF04965    | 70.866 | PF06347    | 5.398  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "detectron2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
